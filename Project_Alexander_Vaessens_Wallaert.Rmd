---
title: | 
        | Project
        | Transplant kidney rejection
        | High Dimensional Data Analysis
author:
  - Jan Alexander^[jan.alexander@ugent.be]
  - Annabel Vaessens^[annabel.vaessens@vub.be]
  - Steven Wallaert^[steven.wallaert@ugent.be]
date: "8/4/2020"
output:
  pdf_document: 
    number_sections: yes
    toc: no
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}

rm(list = ls())

library(latex2exp)
library(tidyverse)
library(boot)
library(PMA)
library(pls)
library(MASS)
library(glmnet)
library(ROCR)
#library(multtest)
#library(RDRToolbox)
library(diffusionMap)
library(car)
library(sgof)
library(tsne)
library(locfdr)

knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)

set.seed(321)
```

```{r load data, eval=TRUE, include=TRUE}
load('RejectionStatus.rda')
load('X_GSE21374.rda')
dim(RejectionStatus)
dim(X_GSE21374)

GeneExpression <- t(X_GSE21374)
GeneExpression_C <- scale(t(X_GSE21374),scale = F) # centered
GeneExpression_S <- scale(t(X_GSE21374),scale = T) # scaled
# vraagje: zie cursus p 74, men geeft aan voor PCA dat
# indien vars in zelfde eenheden zijn (in ons geval is dit zo), dan beter niet schalen.
# Zouden we dan mss beter niet schalen voor PCA?

GeneExpression <-
  GeneExpression[order(as.numeric(row.names(GeneExpression))), ]
RejectionStatus <-
  RejectionStatus[order(as.numeric(RejectionStatus$Patient_ID)), ]

all.equal(row.names(GeneExpression), as.character(RejectionStatus$Patient_ID))
```

# Introduction

The data is loaded as presented in the assignment. The data is loaded as the raw dataset, the centered dataset and the standardized dataset. 

Three research questions are defined:

* How do the 54675 genes vary in terms of their gene expression levels? Is the variability associated with kidney rejection? (only to be answered in a data explorative manner).
* Which genes are differentially expressed between the two kidney rejection groups? You must control the False Discovery Rate at 10%.
* Can the kidney rejection be predicted from the gene expressions? What genes are most important in predicting the kidney transplant rejection? How well does the prediction model perform in terms of predicting rejection status?

# Data exploration

In the complete dataset, `r round(mean(RejectionStatus$Reject_Status) * 100) ` % of the transplanted kidnesy were rejected.

We start of with descriptive statistics

## Descriptives

Several descriptive statistics (mean, sd, median, iqr, min, and max) were calculated for every gene and kidney rejection status combination. This resulted in 2 (accepted vs. rejected) distributions of those statistics across genes. Note that these statistics were only calculated to perform a visual inspection. The resulted plot is presented in figure \ref{descriptives}. From this figure we can see there are, at least on this level, differences between groups. Most notable are the mean and median expression levels which tend to be more centered around the overall (across groups) mean expression levels in the _accepted_ group and more varying in the _rejected_ group. There seems to be more variablity in the measures of dispersion in the _rejected_ group. Finally there are minimal differences between the min/max expression level distributions, perhaps suggesting that gene expression levels in the _rejected_ group are slightly less extreme than in the _accepted_ group.

```{r descriptives, fig.cap="Descriptive statistics across grenes and between groups. Note that the data were centered before calculating the statistics."}
df <- tibble(patient = RejectionStatus$Patient_ID, 
             reject = ifelse(RejectionStatus$Reject_Status == 1, "Rejected", "Accepted"), 
             as.data.frame(GeneExpression_C)) %>%
  pivot_longer(cols = c(-reject,-patient) , names_to = "gene", values_to = "expression")

# calculate summaries
df %>%
  group_by(reject, gene) %>%
  summarise(mean = mean(expression),
            sd = sd(expression),
            min = min(expression),
            max = max(expression),
            median = median(expression),
            iqr = IQR(expression)) -> basics
# long format  
basics %>%
  pivot_longer(-c(reject, gene), names_to = "statistic", values_to = "value") -> basics_long
# fix order of statistics for plot facet
basics_long %>%
  mutate(statistic = factor(statistic, levels = c("mean", "sd", "min",
                                                  "median", "iqr", "max"))) -> basics_long
# plot
ggplot(basics_long, aes(value, fill = reject)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(fill = "Status") + 
  facet_wrap("statistic", scales = "free" ) +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(x="")

```


## Sparse principle components analysis

```{r sparse pca}
SPC.cv(GeneExpression_C)
Gen_spc <- PMA::SPC(GeneExpression_C, K = 2, sumabsv = 5)
Uk <- Gen_spc$u ; Dk <- diag(Gen_spc$d)
Zk <- data.frame(X = Uk %*% Dk, Patient_ID = row.names(GeneExpression_C))
Zk <- merge(Zk, RejectionStatus, by = 'Patient_ID') %>% 
  mutate(Reject_Status = as.factor(Reject_Status))
ggplot(data = Zk, aes(x = X.1, y = X.2, col = Reject_Status)) + 
  geom_point()

rm(Zk, Dk, Uk, X_GSE21374, Gen_spc)
```

Unfortunately, naive sparce principle component analysis does not seem to work well.

## Multi-dimensional scaling:

```{r mds}
GeneExpression_C.svd <- svd(GeneExpression_C)

k <- 3
Uk <- GeneExpression_C.svd$u[,1:k]; Dk <- diag(GeneExpression_C.svd$d[1:k]) 
Vk <-GeneExpression_C.svd$v[,1:k]
Xk <- Uk %*% Dk %*% t(Vk)
Zk <- Uk %*% Dk

rownames(Zk) <- RejectionStatus[[2]]
rownames(Vk) <- colnames(GeneExpression_C)
ColnamesNull <- colnames(GeneExpression_C)
ColnamesNull[]<- "" 

plot(Zk[,1], Zk[,2], col=RejectionStatus$Reject_Status+1,pch=19)
plot(Zk[,2], Zk[,3], col=RejectionStatus$Reject_Status+1,pch=19)
```

In the biplot of the two first dimensions of the svd, no distinction can be made between rejected and accepted kidneys.

```{r plots mds}
totvar <- sum(GeneExpression_C.svd$d^2)/(nrow(GeneExpression_C)-1)
barplot(cumsum(GeneExpression_C.svd$d^2/(nrow(GeneExpression_C)-1)/totvar), names.arg=1:nrow(GeneExpression_C), ylab='cumulative prop. of total variance')
```

In the scree plot above it can be seen that the two first dimensions account for only 25% of the total variance in the dataset and the first three dimensions for 29%. To account for 80% of the total variance, 120 dimensions are needed.


## Sparse LDA

```{r sparse lda}
id.all <- numeric()
loadings <- numeric()
for(i in 1:3){
  
  start <- 1 + (i-1)*18225
  stop <- start + 18224
  
  gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status)
  
  V <- gene_lda$scaling
  
  Z <- GeneExpression_S[,start:stop] %*% V
  
  #plot(Z,col = 1+RejectionStatus$Reject_Status, pch=16, cex=1)
  
  #boxplot(Z~RejectionStatus$Reject_Status)
  
  lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5)
  
  sparse_lda_loadings <- as.vector(coef(lda_loadings))
  
  #SLDA <- GeneExpression_S[,start:stop] %*% sparse_lda_loadings[-1]
  
  #boxplot(SLDA ~ RejectionStatus$Reject_Status)
  
  id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225)
  loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0])
}
```

A sparse LDA was performed to find out which genes contribute the most in discriminating between the two rejection status groups. 
These genes could then be possible targets for further investigation. 
Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` variables).  
In total `r length(id.all)` genes (or `r length(id.all)/dim(GeneExpression)[2]*100` %) had non-zero loadings. 
Figure \ref{slda-loadings} shows a plot of the loadings vs. gene index number. We could decide to only look at the genes with the largest (in absolute value) loadings and only consider those genes whose loadings are more than 2 standard deviations away from 0 (above the red line in figure \ref{slda-loadings}). 
These are the following `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r id.all[which(abs(loadings) > 2*sd(loadings))] %>% colnames(GeneExpression) %>% cat()`. 
Figure \ref{slda-scores} shows the discriminant scores from which we can see there is a substantial overlap between the 2 groups, suggesting that (sparse) LDA is not the best approach.

```{r slda-loadings}
plot(id.all, loadings)
abline(h=0)
abline(h=c(-2,2)*sd(loadings), col =2)
```

```{r slda-scores}
Z <- GeneExpression_S[,id.all] %*% loadings

plot(Z, col=RejectionStatus$Reject_Status+1, pch=16)
```


## LLE (locally linear embedding)

Locally linear embedding (Roweis and Saul, 2000) is a nonlinear dimension reduction method which finds a low-dimensional representation for each points' local neighbourhood by linearly approximating the data in each neighbourhood and returing these coordinates of lower dimension.

(Value of k (number of neighbours) could be optimised)

```{r, eval=FALSE}
lle <- RDRToolbox::LLE(data=GeneExpression_C, dim=3, k=50)
labels = c("first component", "second component", 'third component')
plot(lle[,1],lle[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3D interactive plot: (remove in final version)
#plot <-RDRToolbox::plotDR(data=lle, labels=RejectionStatus[,2], axesLabels=labels)
```
But also with LLE, we cannot differentiate between the accepted and rejected kidneys.
(more on LLE: https://www.stat.cmu.edu/~cshalizi/350/lectures/14/lecture-14.pdf)

## ISOMAP

ISOMAP is a a nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000. It preserves rather the global properties of the data. It uses  multidimensional scaling but then with incorporating the geodesic distances imposed by a weighted graph of the k neighbours of each point.

```{r, eval=FALSE}
IM <- RDRToolbox::Isomap(data=GeneExpression_C, dims=3, k=30)
labelsIM <- c("first component", "second component", "third component")
plot(IM$dim3[,1],IM$dim3[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3d plot (remove in final version)
# RDRToolbox::plotDR(data=IM$dim3, labels=RejectionStatus[,2], axesLabels=labelsIM)

RDRToolbox::Isomap(data=GeneExpression_C, dims=1:10, k=30, plotResiduals=TRUE)
```
The parameter k is varied manually so that the maps are optimal. The value k could be optimised.
With ISOMAP, it is also not possible to make a distinction between the group of accepted and rejected kidneys.

## Sammon mapping

Sammon mapping is also a nonlinear dimension reduction method. The cost function of the Sammon method is similar to that of MDS, except that it is adapted by dividing the squared error in the representation of each pairwise Euclidean distance by the original Euclidean distance in the high-dimensional space. In this way, local structure is better preserved than in MDS.

```{r}
sammon <- MASS::sammon(dist(GeneExpression_C), k=3, niter=100)
plot(sammon$points[,1], sammon$points[,2], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
plot(sammon$points[,2], sammon$points[,3], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```

No distinction between the two groups can be made.

## Diffusion maps

Diffusion mapping is also a nonlinear dimension reduction method based on the eigenvectors and eigenvalues of the data. The global strucure of the data is mapped by integration of local similarities at different scales. It works with probablities of point x randomly walking to y, and is based on the heat diffusion equation.

```{r}
DiffusionMap <- diffusionMap::diffuse(dist(GeneExpression_C), maxdim=3)
plot(DiffusionMap$X, type='p', col=RejectionStatus$Reject_Status+1, pch=19)
```

No distinction between the two groups can be made.

## t-SNE

t-stochastic neighbor embedding is a dimension reduction technique for visualising high dimensional data with a focus on preserving the local structure. The resulting plot can be seen in figure \ref{tsne} and indicates again that a simple distinction between the two groups cannot be made. Yet, there seems to be roughly two groups that differ in heterogeneity: one largely heterogeneous group (upwards left in the plot) and one group that is less heterogeneous, though far from homogeneous (middle to downward right in the plot).

```{r tsne, fig.cap="Two dimensional representation of the data through application of t-SNE"}
gene_dist <- dist(GeneExpression_C)
tsne_Z <- tsne(gene_dist, k = 2, perplexity = 45)
tibble(as.data.frame(tsne_Z), reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", "Rejected"))) %>%
  ggplot(aes(V1, V2, color = reject)) +
  geom_point() +
  theme_classic() +
  labs(color = "Status",
       x="",
       y="")
```



# Differentiating genes between kidney acceptance and rejection

All hypothesis tests are done on the centered, not-standardized data. 

First, a check of the variables follow a normal distribution for both the accepted and rejected kidneys. This is done by checking several QQ plots. 

remark 1: noticed the method is not BH after all. Will fix this later an use Stevens suggestion
remark 2: also test whether the variances of the two groups is unequal? (there will be some variables for sure where variance is unequal, therefore safer to do the Welch t-test rather than the student t-test


```{r QQ plots for normality check, for several random genes}

Rejected=matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==0){
    Rejected <- rbind(Rejected, GeneExpression_C[i,])
  }
}

Accepted=matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==1){
    Accepted <- rbind(Accepted, GeneExpression_C[i,])
  }
}

Selection <- sample(seq(1,ncol(GeneExpression_C)), 15)

for (j in Selection) {
    qqPlot(Rejected[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in rejected kidneys'))
    qqPlot(Accepted[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in accepted kidneys'))
}

```

The QQ plots show that most genes follow the normal distribution, but there are also genes that do not.

remark: maybe replace by a large-scale hypothesis test for normality later on and then decide for the Welch t-test or Wilcoxon Mann-Whitney (now both are in the code) 

(remark on remark: I would rather not do that: On this scale you are guaranteed to have rejections. So in a way you already know the outcome (some are non normal). But if you would allow for a fraction to be non normal, then you have to decide how large that margin would be and then it is again 'guess work'. 
answer to remark: Yes, that is true! And after the professors explaination, we don't really have to be bothered about checking assumptions. So: QQ plots and that's it)

A two-sided two-sample Welch t-test is done on the centered data (not standardized) to determine whether the two groups (rejected and accepted kidneys) can be differentiated based on the gene data, whether the mean  of the two groups is different.The Welch t-test is done because of unequal variances/difference in amount of samples between the two groups.

```{r two-sided two-sample Welch t-test}
# duurt niet lang
p.val <- numeric(ncol(GeneExpression_C))
t.val <- numeric(ncol(GeneExpression_C))

for(i in 1:length(p.val)){
   t_object <- t.test(GeneExpression_C[RejectionStatus$Reject_Status == 0,i],
         GeneExpression_C[RejectionStatus$Reject_Status == 1,i], 
         var.equal = F)
   
  p.val[i] <- t_object$p.value
  t.val[i] <- t_object$statistic
}
hist(p.val, main = "p-values 2 sided Welch t test")
hist(t.val, main = "Test statistic Welch t test", breaks = 50)
# duurt wel lang (+- 5 minuten):
pval_bh <- BH(p.val, alpha = .1)
hist(pval_bh$Adjusted.pvalues)
table(p.val < 0.1)
table(pval_bh$Adjusted.pvalues < 0.1)
pval_bh$FDR
```

Interpretation: after exercise session 5

Another way is to do the BH95 method manually:

```{r tests as in class}
AllTestResults <-matrix(,nrow=ncol(GeneExpression),  
                        ncol=4 ) 
rownames(AllTestResults)=colnames(GeneExpression[1:54675])
colnames(AllTestResults)=c('p-value', 'test-statistic', 'degreeFreedom', 'adjusted p-value')

for (i in seq(1,54675)){
  test=t.test(GeneExpression[,i]~RejectionStatus[,2])
  AllTestResults[i,'p-value'] <- test$p.value
  AllTestResults[i,'test-statistic'] <- test$statistic
  AllTestResults[i,'degreeFreedom'] <- test$parameter
}

```

Histogram of the p-values

```{r histogram p-values}
hist(AllTestResults[,'p-value'], main='Histogram of the p-values', xlab='p-value')
```

This histogram has not the uniform distribution, it has many small p-values. This means that we can reject the null hypotheses for many variables.

Control at FDR 10%

```{r FDR control}
FDRControl<- AllTestResults[order(AllTestResults[,'p-value']),]
Vec <- c(seq(1,54675,by=1)*0.10/54675)
FDRControl <- cbind(FDRControl, Vec)
Discoveries_FDR <- c()
for (i in seq(1,54675))
{
  if (FDRControl[i,'p-value'] < FDRControl[i,'Vec'])
  {
    Discoveries_FDR <- rbind(Discoveries_FDR, c(TRUE))}
  if (FDRControl[i,'p-value'] > FDRControl[i,'Vec'])
  {
    Discoveries_FDR <- rbind(Discoveries_FDR, c(FALSE))}
}

rownames(Discoveries_FDR) <- rownames(FDRControl) 
table(Discoveries_FDR) 
```

We have 18080 accepted hypotheses and 36595 rejected null-hypothyses or discoveries. 
Calculation and plotting of the z-scores and comparison to the normal distribution.

```{r lfdr and z-scores}
z_scores <- c()
for (i in seq(1,54675,1)){
  z_score=qnorm(pt(AllTestResults[i,'test-statistic'],df=AllTestResults[i,'degreeFreedom']))
  z_scores <- cbind(z_scores, z_score)
}

hist(z_scores,breaks=120,prob=T,ylim=c(0,0.39))
curve(dnorm(x, mean=0, sd=1),
      col="darkblue", lwd=2, add=TRUE, yaxt="n")
```

From this plot we can conclude that there are most nonnulls can be discriminated well

```{r lfdr}
fdr <- locfdr(z_scores,plot=0,nulltype=0)$fdr
hist(z_scores,breaks=120,prob=T,ylim=c(0,0.39), ylab=NULL, axes=FALSE, main='lfdr and z-scores')
par(new=TRUE)
plot(z_scores[order(z_scores)],fdr[order(z_scores)],
     type="l",lwd=2,col="red",xlab="z-scores",lty=2,
     ylab="fdr")
```

from this plot we can conclude that we can obtain a small fdr for very small (negative z-scores, smaller than -3) and for very large z-scores (larger than 3).


A Wilcoxon Mann-Whitney test is done on the centered data, to determine whether the two groups follow the same distribution.

```{r Wilcoxon Mann-Whitney test}

```

Interpretation: after exercise session 5

# Prediction of kidney transplant rejection

```{r train test split}
ind_train <-
  sample(seq_len(nrow(RejectionStatus)), size = floor(nrow(RejectionStatus) * 0.80))

Y_train <- as.matrix(RejectionStatus[ind_train, 'Reject_Status'])
X_train <- as.matrix(GeneExpression_C[ind_train,])
Y_test <- as.matrix(RejectionStatus[-ind_train, 'Reject_Status'])
X_test <- as.matrix(GeneExpression_C[-ind_train,])
```

## Lasso regression

```{r lasso}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 1,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance lasso}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 1,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```

This model uses 
`r length(unique(summary(coef(m))[-1,1])) ` 
of the features. 
This is a considerable dimensional reduction.
This is illustrated below. This figure shows the loadings of the 
`r length(unique(summary(coef(m))[-1,1])) ` 
selected values.

```{r summary lasso, fig.width=4, fig.height=3.5, eval=TRUE, echo=FALSE}
plot(
  summary(coef(m))[-1, 1],
  summary(coef(m))[-1, 3],
  cex = 1,
  pch = 3,
  xlab = 'feature' ,
  ylab = TeX(" $ \\hat{\\beta} $ ")
)
```


## Ridge regression

```{r ridge}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 0,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance ridge}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 0,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```


## Principal component regression

```{r PCR CV}
# cost function for CV
MISERR <- function(obs, pred, cutoff = 0.5){
  ypred <- as.numeric(pred > cutoff)
  tab <- table(obs, ypred)
  miserr <- 1 - sum(diag(tab))/sum(tab)
  return(miserr)
}

max.n.comps <- 50 #random nr

cv.glm.pcr <- rep(NA, max.n.comps)
npc.min <- rep(NA, 9)
npc.val <- rep(NA, 9)
X_train.svd <- svd(X_train)

U <- X_train.svd$u
D <- diag(X_train.svd$d)
Z_train <- U %*% D

for (j in seq(1, 9)) {
  for (i in 1:max.n.comps) {
    fitdata <- data.frame(Y_train, Z_train[, 1:i])
    
    mod <- glm(Y_train ~ ., data = fitdata, family = "binomial")
    
    cv.glm.pcr[i] <-
      cv.glm(fitdata, mod, cost = {
        function (obs, pred)
          MISERR(obs, pred, cutoff = j/10)
      }, K = 10)$delta[1]
  }
  
  plot(1:max.n.comps, cv.glm.pcr, type = "l")
  npc.min[j] <- which.min(cv.glm.pcr)
  npc.val[j] <- cv.glm.pcr[npc.min[j]]
  abline(v = npc.min[j], col = 2)
}
npc.m <- npc.min[which.min(npc.val)]
```


```{r PCR model}
V <- X_train.svd$v
Z_test <- X_test %*% V

fitdata <- data.frame(Y_train, Z_train[,1:npc.m])

mod <- glm(Y_train ~ ., data = fitdata)

preddata <- data.frame(Z_test[,1:npc.m])

pred_mod <- prediction(predict(mod, newdata = preddata), Y_test)

perf_mod <- performance(pred_mod, "sens", "fpr")

plot(perf_mod)
```

AUC is `r performance(pred_mod, "auc")@y.values`.
