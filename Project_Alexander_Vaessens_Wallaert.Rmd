---
title: | 
        | Project
        | Transplant kidney rejection
        | High Dimensional Data Analysis
author:
  - Jan Alexander^[jan.alexander@ugent.be]
  - Annabel Vaessens^[annabel.vaessens@vub.be]
  - Steven Wallaert^[steven.wallaert@ugent.be]
date: "8/4/2020"
output:
  pdf_document: 
    number_sections: yes
    toc: no
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}

rm(list = ls())

library(latex2exp)
library(tidyverse)
library(boot)
library(PMA)
library(pls)
library(MASS)
library(glmnet)
library(ROCR)
#library(RDRToolbox)
library(diffusionMap)
library(car)
library(sgof)
library(tsne)
library(locfdr)

knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)

set.seed(321)
```

# Introduction

The data is loaded as the raw dataset, the centered dataset and the standardized dataset.

```{r load data, eval=TRUE, include=TRUE}
load('RejectionStatus.rda')
load('X_GSE21374.rda')
dim(RejectionStatus)
dim(X_GSE21374)

GeneExpression <- t(X_GSE21374)
GeneExpression_C <- scale(t(X_GSE21374),scale = F) # centered
GeneExpression_S <- scale(t(X_GSE21374),scale = T) # scaled

GeneExpression <-
  GeneExpression[order(as.numeric(row.names(GeneExpression))), ]
RejectionStatus <-
  RejectionStatus[order(as.numeric(RejectionStatus$Patient_ID)), ]

all.equal(row.names(GeneExpression), as.character(RejectionStatus$Patient_ID))
```

# Data exploration

In the complete dataset, `r round(mean(RejectionStatus$Reject_Status) * 100) ` % of the transplanted kidneys were rejected. General descriptive statistics and methods for high dimensional data exploiration are used.

## Descriptives

Several descriptive statistics (mean, sd, median, iqr, min, and max) were calculated for every gene and kidney rejection status combination. This resulted in 2 (accepted vs. rejected) distributions of those statistics across genes. Note that these statistics were only calculated to perform a visual inspection. The resulted plot is presented in figure \ref{descriptives}. From this figure we can see there are, at least on this level, differences between groups. Most notable are the mean and median expression levels which tend to be more centered around the overall (across groups) mean expression levels in the _accepted_ group and more varying in the _rejected_ group. There seems to be more variablity in the measures of dispersion in the _rejected_ group. Finally there are minimal differences between the min/max expression level distributions, perhaps suggesting that gene expression levels in the _rejected_ group are slightly less extreme than in the _accepted_ group.

```{r descriptives, fig.cap="Descriptive statistics across grenes and between groups. Note that the data were centered before calculating the statistics."}
df <- tibble(patient = RejectionStatus$Patient_ID, 
             reject = ifelse(RejectionStatus$Reject_Status == 1, "Rejected", "Accepted"), 
             as.data.frame(GeneExpression_C)) %>%
  pivot_longer(cols = c(-reject,-patient) , names_to = "gene", values_to = "expression")

# calculate summaries
df %>%
  group_by(reject, gene) %>%
  summarise(mean = mean(expression),
            sd = sd(expression),
            min = min(expression),
            max = max(expression),
            median = median(expression),
            iqr = IQR(expression)) -> basics
# long format  
basics %>%
  pivot_longer(-c(reject, gene), names_to = "statistic", values_to = "value") -> basics_long
# fix order of statistics for plot facet
basics_long %>%
  mutate(statistic = factor(statistic, levels = c("mean", "sd", "min",
                                                  "median", "iqr", "max"))) -> basics_long
# plot
ggplot(basics_long, aes(value, fill = reject)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(fill = "Status") + 
  facet_wrap("statistic", scales = "free" ) +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(x="")

```

The several methods for exploration of high dimensional data are applied and they can be found in appendices \ref{sparse pca} to \ref{tsne}. No distinction between the accepted and rejected kidney groups could be made with sparse principal component analysis, mulit-dimensional scaling, sparce linear discriminant analysis, locally linear embedding, isomap, sammon mapping, diffusion maps nor t-SNE. This means that the main directions of the gene expression data do not correspond to the groups of the accepted/rejected kidneys.

# Differentiating genes between kidney acceptance and rejection

The hypothesis tests are done on the raw data. First, a visual check whether the variables follow the normal distribution for both the accepted and rejected kidneys is done by checking several QQ plots. 

```{r QQ plots for normality check}

Rejected=matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==0){
    Rejected <- rbind(Rejected, GeneExpression_C[i,])
  }
}

Accepted=matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==1){
    Accepted <- rbind(Accepted, GeneExpression_C[i,])
  }
}

Selection <- sample(seq(1,ncol(GeneExpression_C)), 15)

for (j in Selection) {
    qqPlot(Rejected[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in rejected kidneys'))
    qqPlot(Accepted[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in accepted kidneys'))
}
```

The QQ plots show that most genes follow the normal distribution, but there are also genes that do not. Nevertheless, a two-sided two-sample Welch t-test is done on the raw data to determine whether the two groups (rejected and accepted kidneys) can be differentiated based on the gene data, whether the mean  of the two groups is different. The Welch t-test is chosen because of unequal variances and difference in amount of samples between the two groups, though it seems that not all variables follow the normal distribution. The method of Benjamini and Hochberg (1995) for high-dimensional hypothesis testing is used.

```{r welch t-test}
AllTestResults <-matrix(,nrow=ncol(GeneExpression),  
                        ncol=4 ) 
rownames(AllTestResults)=colnames(GeneExpression[,1:54675])
colnames(AllTestResults)=c('p-value', 'test-statistic', 'degreeFreedom')

for (i in seq(1,54675)){
  test=t.test(GeneExpression[,i]~RejectionStatus[,2])
  AllTestResults[i,'p-value'] <- test$p.value
  AllTestResults[i,'test-statistic'] <- test$statistic
  AllTestResults[i,'degreeFreedom'] <- test$parameter
}
```

Histogram of the p-values

```{r histogram p-values}
hist(AllTestResults[,'p-value'], main='Histogram of the p-values', xlab='p-value')
```

This histogram does not follow the uniform distribution, it has many small p-values. This indicates that for many variables, the null hypothesis can probably be rejected. Next, theBH95 method is done and the FDR is controlled at 10%.

```{r BH95}
FDRControl<- AllTestResults[order(AllTestResults[,'p-value']),]
Vec <- c(seq(1,54675,by=1)*0.10/54675)
FDRControl <- cbind(FDRControl, Vec)
Discoveries_FDR <- c()
for (i in seq(1,54675))
{
  if (FDRControl[i,'p-value'] < FDRControl[i,'Vec'])
  {
    Discoveries_FDR <- rbind(Discoveries_FDR, c(TRUE))}
  if (FDRControl[i,'p-value'] > FDRControl[i,'Vec'])
  {
    Discoveries_FDR <- rbind(Discoveries_FDR, c(FALSE))}
}

rownames(Discoveries_FDR) <- rownames(FDRControl) 
table(Discoveries_FDR) 
```

There are 18080 accepted null hypotheses and 36595 rejected null-hypothyses or discoveries (see `r Discoveries_FDR`). For 36595 of these genes, we conclude that the gene expression differs between the accepted and rejected kidneys. From these discoveries, it is expected to have around 3660 false discoveries (FDR of 10%).

Next, the z-scores are plotted and compared to the local false discovery rate.

```{r z-scores}
z_scores <- c()
for (i in seq(1,54675,1)){
  z_score=qnorm(pt(AllTestResults[i,'test-statistic'],df=AllTestResults[i,'degreeFreedom']))
  z_scores <- cbind(z_scores, z_score)
}
```
```{r lfdr}
fdr <- locfdr(z_scores,plot=0,nulltype=0)$fdr
```
```{r plot z-scores vs lfdr}
hist(z_scores,breaks=120,prob=T,ylim=c(0,0.39), ylab=NULL, axes=FALSE, main='lfdr and z-scores')
par(new=TRUE)
plot(z_scores[order(z_scores)],fdr[order(z_scores)],
     type="l",lwd=2,col="red",xlab="z-scores",lty=2,
     ylab="fdr")
abline(a=0.1, b=0, col='blue')
```

From the figure \ref{plot z-scores vs lfdr} can be concluded that a fdr < 0.1 can be obtained for negative z-scores, smaller than -2.5 and for very large z-scores, larger than 2.5. For these z-scores, it is more likely that when rejecting the null hypotheses, a true discovery is made.

# Prediction of kidney transplant rejection

The dataset is split into a training and test dataset.

```{r train test split}
ind_train <-
  sample(seq_len(nrow(RejectionStatus)), size = floor(nrow(RejectionStatus) * 0.70))

Y_train <- as.matrix(RejectionStatus[ind_train, 'Reject_Status'])
X_train <- as.matrix(GeneExpression_C[ind_train,])
Y_test <- as.matrix(RejectionStatus[-ind_train, 'Reject_Status'])
X_test <- as.matrix(GeneExpression_C[-ind_train,])
```

## Lasso regression

```{r lasso}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 1,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance lasso}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 1,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```

This model uses 
`r length(unique(summary(coef(m))[-1,1])) ` 
of the features. 
This is a considerable dimensional reduction.
This is illustrated below. This figure shows the loadings of the 
`r length(unique(summary(coef(m))[-1,1])) ` 
selected values.

```{r summary lasso, fig.width=4, fig.height=3.5, eval=TRUE, echo=FALSE}
plot(
  summary(coef(m))[-1, 1],
  summary(coef(m))[-1, 3],
  cex = 1,
  pch = 3,
  xlab = 'gene index' ,
  ylab = TeX(" $ \\hat{\\beta} $ ")
)
```


## Ridge regression

```{r ridge}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 0,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance ridge}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 0,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```
AUC is `r performance(pred_m, "auc")@y.values`.

## Principal component regression

```{r PCR CV}
# cost function for CV
MISERR <- function(obs, pred, cutoff = 0.5){
  ypred <- as.numeric(pred > cutoff)
  tab <- table(obs, ypred)
  miserr <- 1 - sum(diag(tab))/sum(tab)
  return(miserr)
}

max.n.comps <- 50 #random nr

cv.glm.pcr <- rep(NA, max.n.comps)
npc.min <- rep(NA, 9)
npc.val <- rep(NA, 9)
X_train.svd <- svd(X_train)

U <- X_train.svd$u
D <- diag(X_train.svd$d)
Z_train <- U %*% D

for (j in seq(1, 9)) {
  for (i in 1:max.n.comps) {
    fitdata <- data.frame(Y_train, Z_train[, 1:i])
    
    mod <- glm(Y_train ~ ., data = fitdata, family = "binomial")
    
    cv.glm.pcr[i] <-
      cv.glm(fitdata, mod, cost = {
        function (obs, pred)
          MISERR(obs, pred, cutoff = j/10)
      }, K = 10)$delta[1]
  }
  
  plot(1:max.n.comps, cv.glm.pcr, type = "l")
  npc.min[j] <- which.min(cv.glm.pcr)
  npc.val[j] <- cv.glm.pcr[npc.min[j]]
  abline(v = npc.min[j], col = 2)
}
npc.m <- npc.min[which.min(npc.val)]
```


```{r PCR model}
V <- X_train.svd$v
Z_test <- X_test %*% V

fitdata <- data.frame(Y_train, Z_train[,1:npc.m])

mod <- glm(Y_train ~ ., data = fitdata)

preddata <- data.frame(Z_test[,1:npc.m])

pred_mod <- prediction(predict(mod, newdata = preddata), Y_test)

perf_mod <- performance(pred_mod, "sens", "fpr")

plot(perf_mod)
```

AUC is `r performance(pred_mod, "auc")@y.values`.

# Conclusions


# Appendices

## Exploration methods for high dimensional data

### Sparse principle components analysis

```{r sparse pca}
SPC.cv(GeneExpression_C)
Gen_spc <- PMA::SPC(GeneExpression_C, K = 2, sumabsv = 5)
Uk <- Gen_spc$u ; Dk <- diag(Gen_spc$d)
Zk <- data.frame(X = Uk %*% Dk, Patient_ID = row.names(GeneExpression_C))
Zk <- merge(Zk, RejectionStatus, by = 'Patient_ID') %>% 
  mutate(Reject_Status = as.factor(Reject_Status))
ggplot(data = Zk, aes(x = X.1, y = X.2, col = Reject_Status)) + 
  geom_point()

rm(Zk, Dk, Uk, X_GSE21374, Gen_spc)
```

Unfortunately, naive sparce principle component analysis cannot be used to make a distinction between the accepted and rejected kidneys.

### Multi-dimensional scaling:

```{r mds}
GeneExpression_C.svd <- svd(GeneExpression_C)

k <- 3
Uk <- GeneExpression_C.svd$u[,1:k]; Dk <- diag(GeneExpression_C.svd$d[1:k]) 
Vk <-GeneExpression_C.svd$v[,1:k]
Xk <- Uk %*% Dk %*% t(Vk)
Zk <- Uk %*% Dk

rownames(Zk) <- RejectionStatus[[2]]
rownames(Vk) <- colnames(GeneExpression_C)
ColnamesNull <- colnames(GeneExpression_C)
ColnamesNull[]<- "" 

plot(Zk[,1], Zk[,2], col=RejectionStatus$Reject_Status+1,pch=19)
plot(Zk[,2], Zk[,3], col=RejectionStatus$Reject_Status+1,pch=19)
```

In the biplots of the three first dimensions of the svd (\ref{mds}), no distinction can be made between rejected and accepted kidneys.

```{r plots mds}
totvar <- sum(GeneExpression_C.svd$d^2)/(nrow(GeneExpression_C)-1)
barplot(cumsum(GeneExpression_C.svd$d^2/(nrow(GeneExpression_C)-1)/totvar), names.arg=1:nrow(GeneExpression_C), ylab='cumulative prop. of total variance')
```

In the scree plot \ref{plots mds} it can be seen that the two first dimensions account for only 25% of the total variance in the dataset and the first three dimensions for 29%. To account for 80% of the total variance, 120 dimensions are needed.

### Sparse LDA

```{r sparse lda}
id.all <- numeric()
loadings <- numeric()
for(i in 1:3){
  
  start <- 1 + (i-1)*18225
  stop <- start + 18224
  
  gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status)
  
  V <- gene_lda$scaling
  
  Z <- GeneExpression_S[,start:stop] %*% V
  
  #plot(Z,col = 1+RejectionStatus$Reject_Status, pch=16, cex=1)
  
  #boxplot(Z~RejectionStatus$Reject_Status)
  
  lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5)
  
  sparse_lda_loadings <- as.vector(coef(lda_loadings))
  
  #SLDA <- GeneExpression_S[,start:stop] %*% sparse_lda_loadings[-1]
  
  #boxplot(SLDA ~ RejectionStatus$Reject_Status)
  
  id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225)
  loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0])
}
```

A sparse LDA was performed to find out which genes contribute the most in discriminating between the two rejection status groups. 
These genes could then be possible targets for further investigation. 
Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` variables).  
In total `r length(id.all)` genes (or `r length(id.all)/dim(GeneExpression)[2]*100` %) had non-zero loadings. 
Figure \ref{slda-loadings} shows a plot of the loadings vs. gene index number. 
We could decide to only look at the genes with the largest (in absolute value) loadings and only consider those genes whose loadings are more than 2 standard deviations away from 0 (above the red line in figure \ref{slda-loadings}). 
These are the following `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r id.all[which(abs(loadings) > 2*sd(loadings))] %>% colnames(GeneExpression) %>% cat()`. 
Figure \ref{slda-scores} shows the discriminant scores from which we can see there is a substantial overlap between the 2 groups, suggesting that (sparse) LDA is not the best approach.

```{r slda-loadings}
plot(id.all, loadings)
abline(h=0)
abline(h=c(-2,2)*sd(loadings), col =2)
```

```{r slda-scores}
Z <- GeneExpression_S[,id.all] %*% loadings

plot(Z, col=RejectionStatus$Reject_Status+1, pch=16)
```

### LLE (locally linear embedding)

Locally linear embedding is a nonlinear dimension reduction method which finds a low-dimensional representation for each points' local neighbourhood by linearly approximating the data in each neighbourhood and returing these coordinates of lower dimension (Roweis and Saul, 2000).

```{r lle, eval=FALSE}
lle <- RDRToolbox::LLE(data=GeneExpression_C, dim=3, k=50)
labels = c("first component", "second component", 'third component')
plot(lle[,1],lle[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
```

From figures \ref{lle} can be seen that no distinction between the accepted and rejected kidneys can be made.

### ISOMAP

ISOMAP is a a nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000. It preserves rather the global properties of the data. It uses  multidimensional scaling but then with incorporating the geodesic distances imposed by a weighted graph of the k neighbours of each point.

```{r ISOMAP, eval=FALSE}
IM <- RDRToolbox::Isomap(data=GeneExpression_C, dims=3, k=30)
labelsIM <- c("first component", "second component", "third component")
plot(IM$dim3[,1],IM$dim3[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3d plot (remove in final version)
# RDRToolbox::plotDR(data=IM$dim3, labels=RejectionStatus[,2], axesLabels=labelsIM)

RDRToolbox::Isomap(data=GeneExpression_C, dims=1:10, k=30, plotResiduals=TRUE)
```

The parameter k is varied manually so that the maps are optimal. From figure \ref{ISOMAP} can be seen that with ISOMAP, it is also not possible to make a distinction between the group of accepted and rejected kidneys.


## Sammon mapping

Sammon mapping is also a nonlinear dimension reduction method. The cost function of the Sammon method is similar to that of MDS, except that it is adapted by dividing the squared error in the representation of each pairwise Euclidean distance by the original Euclidean distance in the high-dimensional space. In this way, local structure is better preserved than in MDS. The result is in figures \ref{sammon}: no distinction can be made between the two groups.

```{r sammon}
sammon <- MASS::sammon(dist(GeneExpression_C), k=3, niter=100)
plot(sammon$points[,1], sammon$points[,2], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
plot(sammon$points[,2], sammon$points[,3], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```

### Diffusion maps

Diffusion mapping is also a nonlinear dimension reduction method based on the eigenvectors and eigenvalues of the data. The global strucure of the data is mapped by integration of local similarities at different scales. It works with probablities of point x randomly walking to y, and is based on the heat diffusion equation.

```{r diffusion}
DiffusionMap <- diffusionMap::diffuse(dist(GeneExpression_C), maxdim=3)
plot(DiffusionMap$X, type='p', col=RejectionStatus$Reject_Status+1, pch=19)
```

From figure \ref{diffusion}, no distinction between the two groups can be made with diffusion maps.

### t-SNE

t-stochastic neighbor embedding is a dimension reduction technique for visualising high dimensional data with a focus on preserving the local structure. The resulting plot can be seen in figure \ref{tsne} and indicates again that a simple distinction between the two groups cannot be made. Yet, there seems to be roughly two groups that differ in heterogeneity: one largely heterogeneous group (upwards left in the plot) and one group that is less heterogeneous, though far from homogeneous (middle to downward right in the plot).

```{r tsne, fig.cap="Two dimensional representation of the data through application of t-SNE"}
gene_dist <- dist(GeneExpression_C)
tsne_Z <- tsne(gene_dist, k = 2, perplexity = 45)
tibble(as.data.frame(tsne_Z), reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", "Rejected"))) %>%
  ggplot(aes(V1, V2, color = reject)) +
  geom_point() +
  theme_classic() +
  labs(color = "Status",
       x="",
       y="")
```

# References

Benjamini Y and Hochberg Y, 1995. Controlling The False Discovery Rate - A Practical And Powerful Approach To Multiple Testing. Journal of the Royal Statistical Society. Series B: Methodological 57, 289-300.

Roweis ST and Saul LK, 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290, 2323-2326.

Tenenbaum JB, De Silva V and Langford JC, 2000. A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319-2323.

