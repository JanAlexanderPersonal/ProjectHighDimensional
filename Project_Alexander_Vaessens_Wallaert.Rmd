---
title: | 
        | Project
        | Transplant kidney rejection
        | High Dimensional Data Analysis
author:
  - Jan Alexander^[jan.alexander@ugent.be]
  - Annabel Vaessens^[annabel.vaessens@vub.be]
  - Steven Wallaert^[steven.wallaert@ugent.be]
date: "`r format(Sys.Date(), '%d %m %Y')`"
output:
  bookdown::pdf_document2: 
    fig_caption: yes
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{amsmath} \usepackage{float} \renewcommand{\abstractname}{Executive summary}
classoption: twocolumn
abstract: "This research examines whether some genes are responsible for a patient's likelihood of rejecting a kidney after transplantation, for the  Gene Expression Omninibus (GEO) dataset. This dataset consists of gene expression levels of 54675 genes from 282 patients. Data exploration methods show that there is no clear way to map the gene expression levels onto the kidney rejection statuses. In other words, only probabilistic claims can be made. From the 54675 genes, 18081 genes are identified as having a differential expression between the group of rejected and the group of accepted kidneys. Kidney rejection can be predicted sufficiently (not without error) from the gene expressions with only as few as 8 genes. These genes are 1552807_a_at, 202270_at, 204014_at, 207735_at, 219777_at, 219990_at, 221658_s_at, and 240413_at. From these, **202270_at, 221658_s_at, 240413_at** are also proposed by the exploratory analysis as potentially predictive for kidney rejection status."
---

```{r setup, include=FALSE}

rm(list = ls())

library(latex2exp)
library(tidyverse)
library(boot)
library(PMA)
library(pls)
library(MASS)
library(glmnet)
library(ROCR)
library(RDRToolbox)
library(diffusionMap)
library(car)
library(tsne)
library(locfdr)
library(xtable)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.width=6, fig.height=4, cache = TRUE)
options(scipen = 9)
options(xtable.comment = FALSE)
options(xtable.sanitize.text.function=identity)
set.seed(321)
```



```{r load data, include=FALSE}
load('RejectionStatus.rda')
load('X_GSE21374.rda')
dim(RejectionStatus)
dim(X_GSE21374)

GeneExpression <- t(X_GSE21374)
GeneExpression_C <- scale(t(X_GSE21374),scale = F) # centered
GeneExpression_S <- scale(t(X_GSE21374),scale = T) # scaled

GeneExpression <-
  GeneExpression[order(as.numeric(row.names(GeneExpression))), ]
RejectionStatus <-
  RejectionStatus[order(as.numeric(RejectionStatus$Patient_ID)), ]

all.equal(row.names(GeneExpression), as.character(RejectionStatus$Patient_ID))
```

# Abbreviations

```{r, results = "asis"}
abbreviation <- c('AUC','IQR', 'LDA', 'LASSO', 'LLE', 'MDS', 'PCA', 'ROC curve', 'sd', 't-SNE')
meaning <-
  c('Area under the (ROC) curve',
    'Interquartile range',
    'Linear discriminant analysis',
    'Least absolute shrinkage and \\\\ & selection operator',
    'Locally linear embedding',
    'Multi dimensional scaling',
    'Principle component analysis',
    'Receiver operating characteristic \\\\ & curve',
    'Standard deviation',
    't-distributed stochastic neighbor \\\\ & embedding')

abb_table <- data.frame(Abbreviation = abbreviation, Meaning = meaning)

# knitr::kable(abb_table,
#              align = 'l',
#              col.names = c('Abbreviation', 'Meaning'),format="latex")
print(xtable(abb_table), include.rownames=FALSE)
```


# Exploratory Analysis

In this section general descriptive statistics are given and multiple methods for high dimensional data exploration are used.

## Basic descriptive summary

In this study `r dim(GeneExpression)[2]` gene expression levels of `r dim(GeneExpression)[1]` samples were analysed. In total, `r sum(RejectionStatus$Reject_Status)` or `r round(mean(RejectionStatus$Reject_Status) * 100) `% of the transplanted kidneys were rejected. 

Several descriptive statistics (mean, sd, median, iqr, min, and max) were calculated for every gene and kidney rejection status combination. This resulted in 2 (accepted vs. rejected) distributions of every statistic across genes. Note that these statistics were only calculated to perform a visual inspection.

The resulted plot is presented in figure \@ref(fig:descriptives). From this figure we can see there are, at least on this level, differences between the two groups. Most notable are the mean and median expression levels which tend to be closer to the overall mean (across groups) expression levels in the _accepted_ group and more varying in the _rejected_ group. There seems to be more variablity in the measures of dispersion in the _rejected_ group. Finally there are minimal differences between the min/max expression level distributions, perhaps suggesting that gene expression levels in the _rejected_ group are slightly less extreme than in the _accepted_ group.

```{r descriptives, echo=FALSE, fig.cap="Descriptive statistics across grenes and between groups."}
df <- tibble(patient = RejectionStatus$Patient_ID, 
             reject = ifelse(RejectionStatus$Reject_Status == 1, "Rejected", "Accepted"), 
             as.data.frame(GeneExpression_C)) %>%
  pivot_longer(cols = c(-reject,-patient) , names_to = "gene", values_to = "expression")

# calculate summaries
df %>%
  group_by(reject, gene) %>%
  summarise(mean = mean(expression),
            sd = sd(expression),
            min = min(expression),
            max = max(expression),
            median = median(expression),
            iqr = IQR(expression)) -> basics
# long format  
basics %>%
  pivot_longer(-c(reject, gene), 
               names_to = "statistic", 
               values_to = "value") -> basics_long
# fix order of statistics for plot facet
basics_long %>%
  mutate(statistic = factor(statistic, 
                            levels = c("mean", "sd", "min",
                                       "median", "iqr", "max"))) -> basics_long
# plot
ggplot(basics_long, aes(value, fill = reject)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(fill = "Rejection status") + 
  facet_wrap("statistic", scales = "free" ) +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(x="centered gene expression level") +
  theme(legend.position = "bottom")
```

## Advanced exploratory analyses

Multiple methods for exploration and visualisation of high dimensional data were applied (sparse PCA, MDS, sparse LDA, LLE, ISOMAP, Sammon Mapping, Diffusion maps, and t-SNE), yet without clear results. Because the sparse LDA gave the best results we discuss the results here and refer to the appendices (section \@ref(mds) to \@ref(tsneheader)) for the results of the other techniques.

```{r sparse lda}
id.all <- numeric()
loadings <- numeric()
variables_per_part <- dim(GeneExpression)[2]/3
for(i in 1:3){
  
  start <- 1 + (i-1)*variables_per_part
  stop <- start + variables_per_part-1
  
  gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status)
  
  V <- gene_lda$scaling
  
  Z <- GeneExpression_S[,start:stop] %*% V
  
  lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5)
  
  sparse_lda_loadings <- as.vector(coef(lda_loadings))
  
  id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225)
  loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0])
}
```

The sparse LDA was performed to find potential candidate genes for future investigation. Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` genes). We considered this approach to be reasonable since we only used it as an exploratory tool. A small simulation was performed to verify its potential as such a tool (see section \@ref(simulation) in the appendices).
In total `r length(id.all)` genes (or `r round(length(id.all)/dim(GeneExpression)[2]*100,1)`%) had non-zero loadings. 

Because this is still a substantial amount, only the genes with loadings in absolute value larger than two standard deviations were further considered ($|v_i| > 2sd_v,$ with $i =\{1,...,116 \}$, $sd_v$ the standard deviation of the loadings, and $v_i$ the ith loading). This resulted in a list of `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r colnames(GeneExpression)[id.all[which(abs(loadings) > 2*sd(loadings))]]`.

By using these gene's loadings we calculated the scores of the linear discriminant for every sample and used these to construct the following graph. 

```{r slda-density, fig.cap="Density plot of linear discriminant scores based on the selected subset of genes."}
best_all <- id.all[which(abs(loadings) > 2*sd(loadings))]
best_116 <- which(abs(loadings) > 2*sd(loadings))

Z <- GeneExpression_C[,best_all] %*% loadings[best_116]


tibble(scores = Z, reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", 
                                                                             "Rejected"))) %>%
  ggplot(aes(x = scores, fill = reject)) +
  geom_density(alpha = 0.4) +
  theme_classic() +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(fill = "Rejection status") +
  theme(legend.position = "bottom") +
  geom_linerange(inherit.aes = F, aes(x=scores), ymin = 0, ymax=0.02 )

```

From this graph we can see that to a degree a distinction can be made, albeit with a substantial overlap.

## Conclusions Exploratory Analysis

Although differences between groups could be found within the data, no articulate distinction between the rejection status groups could be made with any of the used methods. This finding suggests there is relevant information at the genetic level w.r.t. transplant kidney rejection, but more factors need to be taken into account in order to arrive at a better understanding.

The main directions of variability in the gene expression dataset do not coincide with the separation between the rejection status groups. Nevertheless, certain genes were identified as potentially closely related to the differentiation between the two groups using sparse LDA.

# Testing for differential expression

In order to find out which genes are differentially expressed between rejection status groups null hypotheses were tested against alternative hypotheses as follows. 

$$\left.\begin{aligned} H_{0,i}: \mu_{rejected, i} = \mu_{accepted,i} \\ H_{a,i}: \mu_{rejected, i}\neq \mu_{accepted,i} \end{aligned} \right \} \space  \space i = \{1,\dots,54675\} $$

In these hypotheses $\mu_{rejected,i}$ and $\mu_{accepted,i}$ are the population means of the gene expression level of the ith gene in the rejected and accepted kidney rejection group respectively. Before testing, a visual inspection of 30 QQ plots, from 15 randomly drawn variables, was done to assess whether the variables follow a normal distribution for both groups separately. These QQ plots showed that some genes were normally distributed, but also that many genes were not. 

Nevertheless, two-sided Welch t-tests were performed on the uncentered data to determine whether the two groups can be differentiated based on the gene expression level for every gene. The choice for the Welch t-test is motivated by the presence of unequal variances between the two groups, even though not all genes were normally distributed. We included a small random subset 6 QQ plots in the appendix (section \@ref(qq)) so the reader, if she/he wishes, can have a rough idea of the divergence from normality (or the absence thereof).

To address the multiple testing problem at this large scale (54675 simultaneous tests), the FDR is controlled at 0.10 through application of the method of Benjamini and Hochberg (1995).


```{r welch t-test}
AllTestResults <- matrix(nrow=ncol(GeneExpression),  
                        ncol=3 ) 
rownames(AllTestResults) <- colnames(GeneExpression)
colnames(AllTestResults) <- c('p-value', 'test-statistic', 'degreeFreedom')

for (i in seq(1,54675)){
  test <- t.test(GeneExpression[,i]~RejectionStatus[,2])
  AllTestResults[i,'p-value'] <- test$p.value
  AllTestResults[i,'test-statistic'] <- test$statistic
  AllTestResults[i,'degreeFreedom'] <- test$parameter
}
```

To summarise the results we constructed a histogram of the adjusted p-values or q-values (see figure \@ref(fig:histogram)). 

```{r histogram, fig.cap="Histograms of adjusted p-values. The dashed line indicates the threshold"}
adjusted <- p.adjust(AllTestResults[,"p-value"], method = "BH")
rejections <- sum(adjusted < 0.1)

# tibble(gene_index = 1:length(AllTestResults[,1]), pval = AllTestResults[,1],
#        adjusted = adjusted) %>%
#   pivot_longer(-gene_index, names_to = "type", values_to = "p.value") %>%
#   mutate(type = factor(type, levels = c("pval", "adjusted"), 
#                        labels = c("p-value", "q-value"))) -> df.p

df.p <- tibble(adjusted)

ggplot(df.p, aes(adjusted)) +
  geom_histogram(color="grey70", binwidth = 0.025, boundary = 0.1) +
#  facet_grid(.~type) +
  theme_classic() +
  geom_vline(xintercept = 0.1, color = "red", linetype = "dashed") +
  labs(x="q-value",
       y = "frequency") +
  scale_x_continuous(breaks = c(0, 0.1, 0.25, 0.5, 0.75, 1))

```

The histogram shows a non-uniform distribution. More importantly, it tells there are many small values, indicating that for many genes the null hypothesis was rejected. Based on the q-values, there were `r rejections` rejected null hypotheses. As such we conclude that the mean gene expression differs between the accepted and rejected kidney groups for those `r rejections` genes. As the FDR is controlled at 10%, it is expected to have around `r round(rejections*.1,0)` false discoveries.

Next, the normalised test statistics (z-scores) are plotted and compared to the local false discovery rate.

```{r lfdr, fig.cap="Histogram of normalised test statistics. Approximated density (green line overlay). Theoretical null distribution density (blue dashed line overlay). True discovery likelihood indication (purple overlay). "}
z_scores <- qnorm(pt(AllTestResults[,'test-statistic'],df=AllTestResults[i,'degreeFreedom']))
fdr <- locfdr(z_scores,plot=0,nulltype=0)$fdr
z.locfdr <- locfdr(z_scores, nulltype = 0, plot = 1)
```

Fromthe graph in figure \@ref(fig:lfdr) can be concluded that a small lfdr can be obtained for small and large z-scores (a lfdr smaller than 0.2 for z-scores smaller than `r round(z.locfdr$z.2[1] , digits = 2)` and larger than `r round(z.locfdr$z.2[2] , digits = 2)`). For these z-scores, it is more likely that when rejecting the null hypothesis, a true discovery is made.

# Prediction of kidney transplant rejection

The objective of this final part is to construct a classifier for kidney acceptance or rejection based on the measured gene expressions.
Three approaches are compared: LASSO, ridge regression and principle component regression. 
These approaches will be compared based on the AUC.
The final modelling approach is chosen as the one that shows the largest AUC obtained on the test dataset.
The cutoff is chosen based on the F1-score.
The dataset is split into a training (70% of the data) and test dataset (30% of the data).

```{r train test split}
ind_train <-
  sample(seq_len(nrow(RejectionStatus)), size = floor(nrow(RejectionStatus) * 0.70))

Y_train <- as.matrix(RejectionStatus[ind_train, 'Reject_Status'])
X_train <- as.matrix(GeneExpression_C[ind_train,])
Y_test <- as.matrix(RejectionStatus[-ind_train, 'Reject_Status'])
X_test <- as.matrix(GeneExpression_C[-ind_train,])
```

## LASSO regression

```{r lasso, fig.cap="CV plot LASSO regression."}
set.seed(321)
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 1,
    family = 'binomial',
    type.measure = "auc"
  )
mod_lasso <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 1,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_mod_lasso <-
  prediction(predict(
    mod_lasso,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In figure \@ref(fig:lasso), one can see that for $\gamma$ equal to `r round(m.cv$lambda.min, digits = 2)`, the _AUC_ is maximal (`r round(performance(pred_mod_lasso, "auc")@y.values[[1]], digits = 3)`) for the train dataset based on a 10-fold cross-validation over the train dataset.  
The ROC curve, estimated with the cross-validation dataset, is shown in figure \@ref(fig:performance-lasso).

```{r performance-lasso, fig.cap="ROC curve LASSO regression model."}
lasso_selection <- unique(summary(coef(mod_lasso))[-1, 1])-1 # one-off due to intersect
perf <- performance(pred_mod_lasso, 'sens', 'fpr')
plot(perf)
```

This model only uses 
`r length(lasso_selection) ` 
of the genes: `r colnames(GeneExpression)[lasso_selection]`.
This is a considerable dimensional reduction. Table \@ref(tab:lassotab) shows the loadings of the selected values.

```{r lassotable, results="asis"}

genes_lasso <-   str_replace_all(append("intercept", colnames(GeneExpression)[lasso_selection]), "_", "-")
loading_lasso <- summary(coef(mod_lasso))[, 3]

table <- tibble(Genes = genes_lasso, "Parameter value" = loading_lasso)

print(xtable(table, caption = "\\label{tab:lassotab}Coefficients LASSO model", digits = 3),
      table.placement = "H", include.rownames=FALSE)
#knitr::kable(table, caption = "Coefficients LASSO model", format = "latex")
```



## Ridge regression

```{r ridge, fig.cap="CV plot ridge regression."}
set.seed(321)
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 0,
    family = 'binomial',
    type.measure = "auc"
  )
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 0,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the same manner as for the LASSO regression, for the ridge regression a $\gamma$ equal to `r round(m.cv$lambda.min, digits = 2)` and corresponding optimal _AUC_ (`r round(performance(pred_m, "auc")@y.values[[1]], digits = 3)`) was obtained. 

The ROC curve, estimated with the cross-validation dataset, is shown in figure \@ref(fig:performance-ridge).

```{r performance-ridge, fig.cap="ROC curve ridge regression model."}
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```

## Principal component regression

```{r PCR-CV, warning=FALSE, fig.cap="CV plot PCR. The red line shows the number of principal components for which the cross-validete AUC is maximal."}
set.seed(321)
trapezoid_integration <- function(x, y) {
  elements <- (y[-1] + y[-length(y)]) * abs(diff(x))
  sum(elements/2)
}
# cost function for CV
AUC_est <- function(obs, pred){
  obs <- factor(obs, levels = c(0,1))
  # AUC is estimated by calculating the sensitivity and the FPR for different values of cutoff C
  intervals <- 500
  sensitivity <- rep(NA, intervals+1)
  specificity <- rep(NA, intervals+1)
  for(i in seq(0, intervals)){
    cutoff <- i/intervals
    ypred <- factor(as.numeric(pred > cutoff), levels = c(0, 1))
    tab <- table(obs, ypred)
    TN <- tab[1]
    FN <- tab[2]
    FP <- tab[3]
    TP <- tab[4]
    sensitivity[i+1] <- TP / (TP + FN)
    specificity[i+1] <- TN / (FP + TN)
  }
  #plot(1-specificity, sensitivity)
  AUC <- trapezoid_integration(1-specificity, sensitivity)
  #cat(paste0('AUC estimate : ', AUC, '\n'))
  return(AUC)
}

max.n.comps <- 100 #random nr

cv.glm.pcr <- rep(NA, max.n.comps)
X_train.svd <- svd(X_train)

U <- X_train.svd$u
D <- diag(X_train.svd$d)
Z_train <- U %*% D

for (i in 1:max.n.comps) {
  fitdata <- data.frame(Y_train, Z_train[, 1:i])
  
  mod <- glm(Y_train ~ ., data = fitdata, family = "binomial")
  
  cv.glm.pcr[i] <-
    cv.glm(fitdata, mod, cost = AUC_est, K = 10)$delta[1]
}

plot(1:max.n.comps, cv.glm.pcr, type = "l", main = 'Cross validated AUC for PCA regression',
     xlab = "Nr of principal components",
     ylab = "AUC")
npc.max <- which.max(cv.glm.pcr)
npc.val <- cv.glm.pcr[npc.max]
abline(v = npc.max, col = 2)
```


```{r PCR-model, warning=FALSE, fig.cap="ROC curve PCR model."}
V <- X_train.svd$v
Z_test <- X_test %*% V
fitdata <- data.frame(Y_train, Z_train[,1:npc.max])
mod <- glm(Y_train ~ ., data = fitdata)
preddata <- data.frame(Z_test[,1:npc.max])
pred_mod <- prediction(predict(mod, newdata = preddata), Y_test)
perf_mod <- performance(pred_mod, "sens", "fpr")
plot(perf_mod)
```

The AUC for this model is `r round(performance(pred_mod, "auc")@y.values[[1]], digits = 3)`.

## Final model evaluation

```{r final, fig.cap="Cutoff value selection. The red line indicates the maximum F1 score and corresponding cutoff value."}
perf_f1 <- performance(pred_mod_lasso, "f")
plot(perf_f1)
pred <- predict(mod_lasso,newx = X_test,type = 'response')
cutoff <- perf_f1@x.values[[1]][which.max(perf_f1@y.values[[1]])]
abline(v=cutoff, col =2)
```


The model performance in terms of AUC is similar for the 3 models.
Since LASSO regression is the simplest model, this is preferred.
By chosing cutoff c = `r round(cutoff, 2)`, we can achieve a F1-score of `r round(max(perf_f1@y.values[[1]], na.rm = T), 2)`. This is represented in figure \@ref(fig:final). The confusion matrix for the LASSO model with cutoff c = `r round(cutoff, 2)` is shown in table \@ref(tab:confusion).
<!--The confusion matrix shows no false negatives at all, but the number of false positives is high.-->
```{r confusion, results="asis"}

ypred <- factor(as.numeric(pred > cutoff), levels = c(0, 1))
tab <- table(Y_test, ypred)
colnames(tab) <- c('accept pred', 'reject pred')
rownames(tab) <- c('accept obs', 'reject obs')
#knitr::kable(tab, digits = 0, align = 'l', format = "latex")
print(xtable(tab, caption = "\\label{tab:confusion}Confusion matrix final model."), table.placement="H")
```
This confusionmatrix clearly shows a sensitivity of `r round(tab[4] / (tab[2] + tab[4]), digits = 2)`, and a specificity of `r round(tab[1] / (tab[1] + tab[3]), digits = 2)`. 
In short, this predictor seems to strike a balance between both, but the performance is not perfect.
If a person tests positive, there is still a considerable chance the kidney will not be rejected. 
This result could be _explained_ (or at least understood a little better) by looking back at the exploratory analysis. 
There it was already clear that the gene expressions of the patients with rejected kidneys overlap with those of the patients with accepted kidneys. Both are not perfectly separable.
 

# Conclusions

In the exploratory analysis we found that the 2 groups are different, but not fully separable. Through applyication of a procedure based on the sparse LDA (adapted for running on our memory-limited hardware) a few genes were flagged as potentially interesting for further research. From the 54675 genes in the dataset, `r rejections` genes are differentially expressed between the two kidney groups, based on multi-scale Welch t-test at an FDR of 10%.  
The 3 modeling approaches used in this study performed very similar in terms of AUC. The LASSO regression model was selected based on its spareseness. The final model performed well in terms of estimated sensitivity and specificity. Interesting to note is that from the selected genes by the LASSO model, the genes `r colnames(GeneExpression)[intersect(best_all, lasso_selection)]` were also detected by the sparse LDA procedure.


# References

Benjamini Y and Hochberg Y, 1995. Controlling The False Discovery Rate - A Practical And Powerful Approach To Multiple Testing. Journal of the Royal Statistical Society. Series B: Methodological 57, 289-300.

Lafon S and Lee AB, 2006. Diffusion maps and coarse-graining: A unified framework for dimensionality
reduction, graph partitioning, and data set parameterization. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 28, 1393–1403.

Nadler B, Lafon S, Coifman RR, and Kevrekidis IG, 2006. Diffusion maps, spectral clustering and
the reaction coordinates of dynamical systems. Applied and Computational Harmonic Analysis:
Special Issue on Diffusion Maps and Wavelets, 21, 113–127.

Roweis ST and Saul LK, 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290, 2323-2326.

Sammon JW, 1969. A nonlinear mapping for data structure analysis. IEEE Transactions on Computers,
18, 401–409.

Tenenbaum JB, De Silva V and Langford JC, 2000. A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319-2323.

Van Der Maaten L and Hilton G, 2008. Visualising data using t-SNE. Journal of Machine Learning Research, 9, 2579-2605.

\leavevmode\newpage

# Appendices

## Exploration methods for high dimensional data

### Sparse principle components analysis


```{r sparse-pca, results="hide", eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE, fig.cap="sparse PCA"}
#SPC.cv(GeneExpression_C)
Gen_spc <- PMA::SPC(GeneExpression_C, K = 2, sumabsv = 5)
Uk <- Gen_spc$u ; Dk <- diag(Gen_spc$d)
# Zk <- data.frame(X = Uk %*% Dk, Patient_ID = row.names(GeneExpression_C))
# Zk <- merge(Zk, RejectionStatus, by = 'Patient_ID') %>% 
#   mutate(Reject_Status = as.factor(Reject_Status))
# ggplot(data = Zk, aes(x = X.1, y = X.2, col = Reject_Status)) + 
#   geom_point() +
#   theme_classic
Zk <- Uk %*% Dk
plot(Zk, col = RejectionStatus$Reject_Status + 1, pch = 19)
rm(Zk, Dk, Uk, X_GSE21374, Gen_spc)
```

Unfortunately, naive sparce principle component analysis cannot be used to make a distinction between the accepted and rejected kidneys.

### Multi-dimensional scaling: {#mds}

```{r mds, eval=TRUE, fig.cap="Biplot (not showing the vectors) MDS, dimensions 1 and 2."}
GeneExpression_C.svd <- svd(GeneExpression_C)

k <- 3
Uk <- GeneExpression_C.svd$u[,1:k]; Dk <- diag(GeneExpression_C.svd$d[1:k]) 
Vk <-GeneExpression_C.svd$v[,1:k]
Xk <- Uk %*% Dk %*% t(Vk)
Zk <- Uk %*% Dk

rownames(Zk) <- RejectionStatus[[2]]
rownames(Vk) <- colnames(GeneExpression_C)
ColnamesNull <- colnames(GeneExpression_C)
ColnamesNull[]<- "" 

plot(Zk[,1], Zk[,2], col=RejectionStatus$Reject_Status+1,pch=19)
```

```{r mds2, fig.cap="Biplot (not showing the vectors) MDS, dimensions 2 and 3."}
plot(Zk[,2], Zk[,3], col=RejectionStatus$Reject_Status+1,pch=19)
```


In the biplots of the three first dimensions of the svd, no distinction can be made between rejected and accepted kidneys.

```{r plotsmds, fig.cap="Scree plot MDS.", eval=TRUE}
totvar <- sum(GeneExpression_C.svd$d^2)/(nrow(GeneExpression_C)-1)
barplot(cumsum(GeneExpression_C.svd$d^2/(nrow(GeneExpression_C)-1)/totvar), names.arg=1:nrow(GeneExpression_C), ylab='cumulative prop. of total variance')
```

In the scree plot \@ref(fig:plotsmds) it can be seen that the two first dimensions account for only 25% of the total variance in the dataset and the first three dimensions for 29%. To account for 80% of the total variance, 120 dimensions are needed.


<!-- ### Sparse LDA -->

<!-- ```{r sparse lda} -->
<!-- id.all <- numeric() -->
<!-- loadings <- numeric() -->
<!-- for(i in 1:3){ -->

<!--   start <- 1 + (i-1)*18225 -->
<!--   stop <- start + 18224 -->

<!--   gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status) -->

<!--   V <- gene_lda$scaling -->

<!--   Z <- GeneExpression_S[,start:stop] %*% V -->

<!--   #plot(Z,col = 1+RejectionStatus$Reject_Status, pch=16, cex=1) -->

<!--   #boxplot(Z~RejectionStatus$Reject_Status) -->

<!--   lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5) -->

<!--   sparse_lda_loadings <- as.vector(coef(lda_loadings)) -->

<!--   #SLDA <- GeneExpression_S[,start:stop] %*% sparse_lda_loadings[-1] -->

<!--   #boxplot(SLDA ~ RejectionStatus$Reject_Status) -->

<!--   id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225) -->
<!--   loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0]) -->
<!-- } -->
<!-- ``` -->

<!-- A sparse LDA was performed to find out which genes contribute the most in discriminating between the two rejection status groups.  -->
<!-- These genes could then be possible targets for further investigation.  -->
<!-- Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` variables).   -->
<!-- In total `r length(id.all)` genes (or `r length(id.all)/dim(GeneExpression)[2]*100` %) had non-zero loadings.  -->
<!-- Figure \ref{slda-loadings} shows a plot of the loadings vs. gene index number.  -->
<!-- We could decide to only look at the genes with the largest (in absolute value) loadings and only consider those genes whose loadings are more than 2 standard deviations away from 0 (above the red line in figure \ref{slda-loadings}).  -->
<!-- These are the following `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r id.all[which(abs(loadings) > 2*sd(loadings))] %>% colnames(GeneExpression) %>% cat()`.  -->
<!-- Figure \ref{slda-scores} shows the discriminant scores from which we can see there is a substantial overlap between the 2 groups, suggesting that (sparse) LDA is not the best approach. -->

<!-- ```{r slda-loadings} -->
<!-- plot(id.all, loadings) -->
<!-- abline(h=0) -->
<!-- abline(h=c(-2,2)*sd(loadings), col =2) -->
<!-- ``` -->

<!-- ```{r slda-scores} -->
<!-- Z <- GeneExpression_S[,id.all] %*% loadings -->

<!-- plot(Z, col=RejectionStatus$Reject_Status+1, pch=16) -->
<!-- ``` -->

### LLE

Locally linear embedding described by Roweis and Saul (2000) was performed. From the next figures can be seen that no distinction between the accepted and rejected kidneys can be made.

```{r lle, fig.cap="LLE, dimensions 1 and 2.", eval=TRUE}
lle <- RDRToolbox::LLE(data=GeneExpression_C, dim=3, k=50)
labels = c("first component", "second component", 'third component')
plot(lle[,1],lle[,2],col=RejectionStatus$Reject_Status+1,pch=19)
#plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
```

```{r lle2, fig.cap="LLE, dimensions 2 and 3.", eval=TRUE}

plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
```



### ISOMAP

ISOMAP presented by Tenenbaum, Silva and Langford in 2000 is performed. The parameter k is varied manually so that the maps are optimal. From figure \@ref(fig:ISOMAP) can be seen that with ISOMAP, it is also not possible to make a distinction between the group of accepted and rejected kidneys.

```{r ISOMAP, fig.cap="ISOMAP, dimensions 1 and 2.", eval=TRUE, echo=FALSE}
IM <- RDRToolbox::Isomap(data=GeneExpression_C, dims=3, k=30)
labelsIM <- c("first component", "second component", "third component")
plot(IM$dim3[,1],IM$dim3[,2],col=RejectionStatus$Reject_Status+1,pch=19)
#plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3d plot (remove in final version)
# RDRToolbox::plotDR(data=IM$dim3, labels=RejectionStatus[,2], axesLabels=labelsIM)

#RDRToolbox::Isomap(data=GeneExpression_C, dims=1:10, k=30, plotResiduals=TRUE)
```

```{r ISOMAP2, fig.cap="ISOMAP, dimensions 2 and 3.", eval=TRUE, echo=FALSE}

plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)

```

### Sammon mapping

Sammon mapping presented by Sammon (1969). The result is in figures \@ref(fig:sammon): no distinction can be made between the two groups.

```{r sammon, results="hide", fig.cap="Sammon mapping dimensions 1 and 2.", eval=TRUE, echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
sammon <- MASS::sammon(dist(GeneExpression_C), k=3, niter=100)
plot(sammon$points[,1], sammon$points[,2], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```

```{r sammon2, results="hide", fig.cap="Sammon mapping dimensions 2 and 3."}
plot(sammon$points[,2], sammon$points[,3], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```


### Diffusion maps

Diffusion mapping was presented by Nadler et al. (2006) and LAfon and Lee (2006). From figure \@ref(fig:diffusion), no distinction between the two groups can be made with diffusion maps.

```{r diffusion, results="hide", fig.cap="Diffusion map", eval=TRUE}
DiffusionMap <- diffusionMap::diffuse(dist(GeneExpression_C), maxdim=3)
plot(DiffusionMap$X, type='p', col=RejectionStatus$Reject_Status+1, pch=19)
```

### t-SNE {#tsneheader}

t-stochastic neighbor embedding is presented by Van Den Maaten and Hilton (2008). The resulting plot can be seen in figure \@ref(fig:tsne) and indicates again that a simple distinction between the two groups cannot be made. Yet, there seems to be roughly two groups that differ in heterogeneity: one largely heterogeneous group and one group that is less heterogeneous, though far from homogeneous.

```{r tsne, fig.cap="Two dimensional representation of the data through application of t-SNE", eval=TRUE}
gene_dist <- dist(GeneExpression_C)
tsne_Z <- tsne(gene_dist, k = 2, perplexity = 45)
# tibble(as.data.frame(tsne_Z), reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", "Rejected"))) %>%
#   ggplot(aes(V1, V2, color = reject)) +
#   geom_point() +
#   theme_classic() +
#   labs(color = "Status",
#        x="",
#        y="")
plot(tsne_Z, col = 1 + RejectionStatus$Reject_Status, pch = 19)
```



```{r, rsults="hide"}

# helper function
slda <-  function (x,y){
  x_lda <- MASS::lda(x, grouping = factor(y))
  Z_orig <- x %*% x_lda$scaling
  
  x_loadings <- cv.glmnet(x, Z_orig, alpha = 0.5, nfolds = 5)
  
  as.vector(coef(x_loadings))[-1]
}

#nr of repetitions
nsims <- 50

#output vars
cors <- hit <- sim_sens <- prop_notinfull <- successes <- matrix(rep(NA, nsims), ncol = 1)

# high dimensional setting, even when split in 3 parts
n <- 33
p <- 102

# this is for finding a cutoff value to use in the data generation so that approx 27% would be labeled 1
find_treshold <- function(c){
  res <- numeric(1000)
  for(i in 1:1000) {
    res[i] <-mean(apply(mvtnorm::rmvnorm(n, mean = c(0,1), sigma = matrix(c(1,0.5, 0.5, 1), ncol =2)), 1, sum) >c)
  }
  
  mean((res - 0.27)**2)
}

cutoff_value <- optimise(find_treshold, lower = 1, upper = 3)$minimum


for(i in 1:nsims){
    
    original <- matrix(rnorm(n*(p-2)),ncol = p-2)
    original <- cbind(mvtnorm::rmvnorm(n, mean = c(0,1), sigma = matrix(c(1,0.7, 0.7, 1), ncol =2)), original)
    orig_y <- ifelse(apply(original[,1:2], 1, sum) > cutoff_value, 1 ,0)

    parts <- c(slda(original[,1:(p/3)], orig_y), 
               slda(original[,((p/3)+1):(2*p/3)], orig_y),
               slda(original[,((2*p/3)+1):p], orig_y))
    full <- slda(original, orig_y)
    
    
    # correlation
    cors[i] <- cor(full, parts)
    # at least one hit
    hit[i] <- length(intersect(which(full>0), which(parts>0))) > 0
    # what proportion of "full detections" were also detected by split method? the more the better
    sim_sens[i] <- length(intersect(which(full>0), which(parts>0)))/sum(full>0)
    
    # what proportion of "split detections" were not discovered by full method? the less the better
    prop_notinfull[i] <- (sum(parts > 0) - length(intersect(which(full>0), which(parts>0))))/sum(parts>0)
    
    successes[i] <- sum(orig_y)
  }

```
### Simulation sparse LDA {#simulation}
In order to have an idea whether sparse LDA split in 3 parts can be used as an exploratory tool we ran a small simulation. We simulated high dimensional data in such a way that also the split parts were high dimensional (n = 33, p = 102). We kept the number of observations and variables as low as possible to make it computationally feasible and still high enough to be able to get some insights from the results. We constructed the data in such a manner that only 2 variables were predictive of the response (note this is `r round(2/102*100,1)`%, which is different from the real data: `r round(length(id.all)/dim(GeneExpression)[2]*100,1)`%). The response was constructed in such a way that approximately 27% were successes (allowing for variation over simulation repetitions).  
The simulation was repeated 50 times and we looked at following measures: correlation between coefficients (mean correlation `r round(mean(cors, na.rm=T),2)`, .25^th^ and .75^th^ quantiles (`r round(quantile(cors, c(.25,.75), na.rm = T),2)`, higher is better), `r sum(is.na(cors))` correlations were not computable because one or both method(s) gave no coefficients), proportion of simulations in which at least 1 variable detected by the full method was also detected by the split method (`r round(mean(hit),2)`, higher is better), proportion of variables detected by the full method that also were detected by the split method (mean proportion `r round(mean(sim_sens, na.rm=T),2)`, .25^th^ and .75^th^ quantiles (`r round(quantile(sim_sens, c(.25,.75), na.rm = T),2)`, higher is better), and proportion of variables detected by the split method that weren't detected by the full method (mean proportion =`r round(mean(prop_notinfull, na.rm=T),2)`, .25^th^ and .75^th^ quantiles (`r round(quantile(prop_notinfull, c(.25,.75), na.rm = T),2)`, lower is better).  
Although this not a formal way of making a comparison, with these results we feel confident enough to use this adapted approach, albeit only as an exploratory tool. Please note that we don't claim that the adapted method *is* valid.


## QQ plots {#qq}

```{r QQ plots for normality check, eval=TRUE}

# Rejected <- matrix(ncol=ncol(GeneExpression_C))
# for (i in seq(1,nrow(RejectionStatus), 1)){
#   if (RejectionStatus[[i,2]]==0){
#     Rejected <- rbind(Rejected, GeneExpression_C[i,])
#   }
# }
# 
# Accepted <- matrix(ncol=ncol(GeneExpression_C))
# for (i in seq(1,nrow(RejectionStatus),1)){
#   if (RejectionStatus[[i,2]]==1){
#     Accepted <- rbind(Accepted, GeneExpression_C[i,])
#   }
# }
# 
# Selection <- sample(seq(1, ncol(GeneExpression_C)), 3)
nvars <- 3
id <- sample(ncol(GeneExpression), nvars)
for (j in 1:nvars) {
  
    qqPlot(GeneExpression[RejectionStatus$Reject_Status == 1,id[j]], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in rejected kidneys'), ylab = "empirical quantiles")
    qqPlot(GeneExpression[RejectionStatus$Reject_Status == 0,id[j]], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in accepted kidneys'), ylab = "empirical quantiles")
}


```

