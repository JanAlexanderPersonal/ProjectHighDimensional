---
title: | 
        | Project
        | Transplant kidney rejection
        | High Dimensional Data Analysis
author:
  - Jan Alexander^[jan.alexander@ugent.be]
  - Annabel Vaessens^[annabel.vaessens@vub.be]
  - Steven Wallaert^[steven.wallaert@ugent.be]
date: "8/4/2020"
output:
  pdf_document: 
    number_sections: yes
    toc: no
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}

rm(list = ls())

library(latex2exp)
library(tidyverse)
library(boot)
library(PMA)
library(pls)
library(MASS)
library(glmnet)
library(ROCR)
library(RDRToolbox)
library(diffusionMap)
library(car)
library(tsne)
library(locfdr)

knitr::opts_chunk$set(echo = FALSE, fig.width=6, fig.height=4)

set.seed(321)
```

# Technical Report {-}

Readers who are primarily interested in the overall conclusions without too much details on the methods can easily consult the **summarised results**, which accompanies this document. This is the technical report containing a more detailed discussion of the used methods and obtained results.

```{r load data, include=FALSE}
load('RejectionStatus.rda')
load('X_GSE21374.rda')
dim(RejectionStatus)
dim(X_GSE21374)

GeneExpression <- t(X_GSE21374)
GeneExpression_C <- scale(t(X_GSE21374),scale = F) # centered
GeneExpression_S <- scale(t(X_GSE21374),scale = T) # scaled

GeneExpression <-
  GeneExpression[order(as.numeric(row.names(GeneExpression))), ]
RejectionStatus <-
  RejectionStatus[order(as.numeric(RejectionStatus$Patient_ID)), ]

all.equal(row.names(GeneExpression), as.character(RejectionStatus$Patient_ID))
```

# Exploratory Analysis

In this section general descriptive statistics are given and multiple methods for high dimensional data exploration are used.

## Basic descriptive summary

In the complete dataset, `r round(mean(RejectionStatus$Reject_Status) * 100) `% of the transplanted kidneys were rejected. 

Several descriptive statistics (mean, sd, median, iqr, min, and max) were calculated for every gene and kidney rejection status combination. This resulted in 2 (accepted vs. rejected) distributions of every statistic across genes. Note that these statistics were only calculated to perform a visual inspection.

The resulted plot is presented in figure \ref{descriptives}. From this figure we can see there are, at least on this level, differences between groups. Most notable are the mean and median expression levels which tend to be closer to the  overall mean (across groups) expression levels in the _accepted_ group and more varying in the _rejected_ group. There seems to be more variablity in the measures of dispersion in the _rejected_ group. Finally there are minimal differences between the min/max expression level distributions, perhaps suggesting that gene expression levels in the _rejected_ group are slightly less extreme than in the _accepted_ group.

```{r descriptives, echo=FALSE, fig.cap="Descriptive statistics across grenes and between groups. Note that the data were centered before calculating the statistics."}
df <- tibble(patient = RejectionStatus$Patient_ID, 
             reject = ifelse(RejectionStatus$Reject_Status == 1, "Rejected", "Accepted"), 
             as.data.frame(GeneExpression_C)) %>%
  pivot_longer(cols = c(-reject,-patient) , names_to = "gene", values_to = "expression")

# calculate summaries
df %>%
  group_by(reject, gene) %>%
  summarise(mean = mean(expression),
            sd = sd(expression),
            min = min(expression),
            max = max(expression),
            median = median(expression),
            iqr = IQR(expression)) -> basics
# long format  
basics %>%
  pivot_longer(-c(reject, gene), 
               names_to = "statistic", 
               values_to = "value") -> basics_long
# fix order of statistics for plot facet
basics_long %>%
  mutate(statistic = factor(statistic, 
                            levels = c("mean", "sd", "min",
                                       "median", "iqr", "max"))) -> basics_long
# plot
ggplot(basics_long, aes(value, fill = reject)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(fill = "Status") + 
  facet_wrap("statistic", scales = "free" ) +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(x="")
```

## Advanced exploratory analyses

Multiple methods for exploration and visualisation of high dimensional data were applied (sparse PCA, MDS, sparse LDA, LLE, ISOMAP, Sammon Mapping, Diffusion maps, and t-SNE), yet without clear results. Because the sparse LDA gave the best results we discuss the results here and refer to the appendices \ref{MDS} to \ref{tsne} for the results of the other techniques.

```{r sparse lda, eval=FALSE, echo=FALSE}
id.all <- numeric()
loadings <- numeric()
variables_per_part <- dim(GeneExpression)[2]/3
for(i in 1:3){
  
  start <- 1 + (i-1)*variables_per_part
  stop <- start + variables_per_part-1
  
  gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status)
  
  V <- gene_lda$scaling
  
  Z <- GeneExpression_S[,start:stop] %*% V
  
  lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5)
  
  sparse_lda_loadings <- as.vector(coef(lda_loadings))
  
  id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225)
  loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0])
}
```

The sparse LDA was performed to find potential candidate genes for future investigation. Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` genes). We considered this approach to be valid since we only used it as an exploratory tool.
In total `r length(id.all)` genes (or `r length(id.all)/dim(GeneExpression)[2]*100`%) had non-zero loadings. 

Because this still is a substantial amount, only the genes with loadings in absolute value larger than two standard deviations were further considered ($|v_i| > 2sd(v),$ with $i =\{1,...,116 \}$, where $v_i$ is the ith loading). This resulted in a list of `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes. The list of genes can be found in appendix \ref{sparse LDA}.


TO DO: plot invoegen, boxplot of scatterplot, + kiezen tussen ofwel op basis van 116 genen of best subset.


## Conclusions Exploratory Analysis

No articulate distinction between the rejection status groups could be made with any of the used methods. This suggests that the main directions of the gene expression data do not correspond to the groups of the accepted/rejected kidneys.

Overall the results indicate that there certainly is relevant information at the genetic level w.r.t. transplant kidney rejection, in the sense that it was possible to make a certain distinction between groups, but not without a substantial overlap. The latter suggests that a relatively large part of the information given in the data set is not informative for the distinction of transplant kidney rejection.

# Differentiating genes between kidney acceptance and rejection

The hypothesis tests were performed on the uncentered data. Before testing, a visual inspection of several QQ-plots was done to verify whether the variables follow a normal distribution for both the accepted and rejected groups. The interested reader can find a few of these QQ-plots in the appendix "QQ-plots".

These QQ-plots showed that some genes were normally distributed, but also that some genes are not. Nevertheless, two-sided two-sample Welch t-tests were done on the uncentered data to determine whether the two groups can be differentiated based on the gene expression level for every gene. The Welch t-test was chosen because of unequal variances and difference in sample size between the two groups, even though it seems that not all genes were normally distributed. 

The null hypotheses are $\mu_{rejected, i} = \mu_{accepted,i}$ with $i = \{1,...,54675\}$, and the alternative hypotheses are $\mu_{rejected, i}\neq \mu_{accepted,i}$ with $i = \{1,...,54675\}$.

To account for the multiple testing problem at this scale (54675 simultaneous hypothesis tests), the FDR is controlled at 0.10 through application of the method of Benjamini and Hochberg (1995).


```{r welch t-test}
AllTestResults <- matrix(nrow=ncol(GeneExpression),  
                        ncol=3 ) 
rownames(AllTestResults) <- colnames(GeneExpression[,1:54675])
colnames(AllTestResults) <- c('p-value', 'test-statistic', 'degreeFreedom')

for (i in seq(1,54675)){
  test <- t.test(GeneExpression[,i]~RejectionStatus[,2])
  AllTestResults[i,'p-value'] <- test$p.value
  AllTestResults[i,'test-statistic'] <- test$statistic
  AllTestResults[i,'degreeFreedom'] <- test$parameter
}
```

Histogram of the p-values

```{r histogram p-values}
hist(AllTestResults[,'p-value'], main='Histogram of the p-values', xlab='p-value')
```

This histogram does not follow the uniform distribution, it has many small p-values. This indicates that for many variables, the null hypothesis can probably be rejected. Next, theBH95 method is done and the FDR is controlled at 10%.

```{r}
adjusted <- p.adjust(AllTestResults[,"p-value"], method = "BH")

rejections <- sum(adjusted < 0.1)
```


There were `r rejections` rejected null hypotheses. As such we conclude that the gene expression differs between the accepted and rejected kidneys for those `r rejections` genes. As the FDR is controlled at 10%, it is expected to have around `r round(rejections*.1,0)` false discoveries.

Next, the z-scores are plotted and compared to the local false discovery rate.

```{r z-scores}
z_scores <- qnorm(pt(AllTestResults[,'test-statistic'],df=AllTestResults[i,'degreeFreedom']))
fdr <- locfdr(z_scores,plot=0,nulltype=0)$fdr

hist(z_scores,breaks=120,prob=T,ylim=c(0,0.39), ylab=NULL, axes=FALSE, main='lfdr and z-scores')
par(new=TRUE)
plot(z_scores[order(z_scores)],fdr[order(z_scores)],
     type="l",lwd=2,col="red",xlab="z-scores",lty=2,
     ylab="fdr")
abline(a=0.1, b=0, col='blue')
```



From the figure \ref{plot z-scores vs lfdr} can be concluded that a fdr < 0.1 can be obtained for negative z-scores, smaller than -2.5 and for very large z-scores, larger than 2.5. For these z-scores, it is more likely that when rejecting the null hypotheses, a true discovery is made.

# Prediction of kidney transplant rejection

The dataset is split into a training and test dataset.

```{r train test split}
ind_train <-
  sample(seq_len(nrow(RejectionStatus)), size = floor(nrow(RejectionStatus) * 0.70))

Y_train <- as.matrix(RejectionStatus[ind_train, 'Reject_Status'])
X_train <- as.matrix(GeneExpression_C[ind_train,])
Y_test <- as.matrix(RejectionStatus[-ind_train, 'Reject_Status'])
X_test <- as.matrix(GeneExpression_C[-ind_train,])
```

## Lasso regression

```{r lasso}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 1,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance lasso}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 1,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```

This model uses 
`r length(unique(summary(coef(m))[-1,1])) ` 
of the features. 
This is a considerable dimensional reduction.
This is illustrated below. This figure shows the loadings of the 
`r length(unique(summary(coef(m))[-1,1])) ` 
selected values.

```{r summary lasso, fig.width=4, fig.height=3.5, eval=TRUE, echo=FALSE}
plot(
  summary(coef(m))[-1, 1],
  summary(coef(m))[-1, 3],
  cex = 1,
  pch = 3,
  xlab = 'gene index' ,
  ylab = TeX(" $ \\hat{\\beta} $ ")
)
```


## Ridge regression

```{r ridge}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 0,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance ridge}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 0,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```
AUC is `r performance(pred_m, "auc")@y.values`.

## Principal component regression

```{r PCR CV}
# cost function for CV
MISERR <- function(obs, pred, cutoff = 0.5){
  ypred <- as.numeric(pred > cutoff)
  tab <- table(obs, ypred)
  miserr <- 1 - sum(diag(tab))/sum(tab)
  return(miserr)
}

max.n.comps <- 50 #random nr

cv.glm.pcr <- rep(NA, max.n.comps)
npc.min <- rep(NA, 9)
npc.val <- rep(NA, 9)
X_train.svd <- svd(X_train)

U <- X_train.svd$u
D <- diag(X_train.svd$d)
Z_train <- U %*% D

for (j in seq(1, 9)) {
  for (i in 1:max.n.comps) {
    fitdata <- data.frame(Y_train, Z_train[, 1:i])
    
    mod <- glm(Y_train ~ ., data = fitdata, family = "binomial")
    
    cv.glm.pcr[i] <-
      cv.glm(fitdata, mod, cost = {
        function (obs, pred)
          MISERR(obs, pred, cutoff = j/10)
      }, K = 10)$delta[1]
  }
  
  plot(1:max.n.comps, cv.glm.pcr, type = "l")
  npc.min[j] <- which.min(cv.glm.pcr)
  npc.val[j] <- cv.glm.pcr[npc.min[j]]
  abline(v = npc.min[j], col = 2)
}
npc.m <- npc.min[which.min(npc.val)]
```


```{r PCR model}
V <- X_train.svd$v
Z_test <- X_test %*% V

fitdata <- data.frame(Y_train, Z_train[,1:npc.m])

mod <- glm(Y_train ~ ., data = fitdata)

preddata <- data.frame(Z_test[,1:npc.m])

pred_mod <- prediction(predict(mod, newdata = preddata), Y_test)

perf_mod <- performance(pred_mod, "sens", "fpr")

plot(perf_mod)
```

AUC is `r performance(pred_mod, "auc")@y.values`.

# Conclusions


# Appendices

## Exploration methods for high dimensional data

### Sparse principle components analysis

```{r sparse pca}
SPC.cv(GeneExpression_C)
Gen_spc <- PMA::SPC(GeneExpression_C, K = 2, sumabsv = 5)
Uk <- Gen_spc$u ; Dk <- diag(Gen_spc$d)
Zk <- data.frame(X = Uk %*% Dk, Patient_ID = row.names(GeneExpression_C))
Zk <- merge(Zk, RejectionStatus, by = 'Patient_ID') %>% 
  mutate(Reject_Status = as.factor(Reject_Status))
ggplot(data = Zk, aes(x = X.1, y = X.2, col = Reject_Status)) + 
  geom_point()

rm(Zk, Dk, Uk, X_GSE21374, Gen_spc)
```

Unfortunately, naive sparce principle component analysis cannot be used to make a distinction between the accepted and rejected kidneys.

### Multi-dimensional scaling:

```{r mds}
GeneExpression_C.svd <- svd(GeneExpression_C)

k <- 3
Uk <- GeneExpression_C.svd$u[,1:k]; Dk <- diag(GeneExpression_C.svd$d[1:k]) 
Vk <-GeneExpression_C.svd$v[,1:k]
Xk <- Uk %*% Dk %*% t(Vk)
Zk <- Uk %*% Dk

rownames(Zk) <- RejectionStatus[[2]]
rownames(Vk) <- colnames(GeneExpression_C)
ColnamesNull <- colnames(GeneExpression_C)
ColnamesNull[]<- "" 

plot(Zk[,1], Zk[,2], col=RejectionStatus$Reject_Status+1,pch=19)
plot(Zk[,2], Zk[,3], col=RejectionStatus$Reject_Status+1,pch=19)
```

In the biplots of the three first dimensions of the svd (\ref{mds}), no distinction can be made between rejected and accepted kidneys.

```{r plots mds}
totvar <- sum(GeneExpression_C.svd$d^2)/(nrow(GeneExpression_C)-1)
barplot(cumsum(GeneExpression_C.svd$d^2/(nrow(GeneExpression_C)-1)/totvar), names.arg=1:nrow(GeneExpression_C), ylab='cumulative prop. of total variance')
```

In the scree plot \ref{plots mds} it can be seen that the two first dimensions account for only 25% of the total variance in the dataset and the first three dimensions for 29%. To account for 80% of the total variance, 120 dimensions are needed.

### Sparse LDA

```{r sparse lda}
id.all <- numeric()
loadings <- numeric()
for(i in 1:3){
  
  start <- 1 + (i-1)*18225
  stop <- start + 18224
  
  gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status)
  
  V <- gene_lda$scaling
  
  Z <- GeneExpression_S[,start:stop] %*% V
  
  #plot(Z,col = 1+RejectionStatus$Reject_Status, pch=16, cex=1)
  
  #boxplot(Z~RejectionStatus$Reject_Status)
  
  lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5)
  
  sparse_lda_loadings <- as.vector(coef(lda_loadings))
  
  #SLDA <- GeneExpression_S[,start:stop] %*% sparse_lda_loadings[-1]
  
  #boxplot(SLDA ~ RejectionStatus$Reject_Status)
  
  id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225)
  loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0])
}
```

A sparse LDA was performed to find out which genes contribute the most in discriminating between the two rejection status groups. 
These genes could then be possible targets for further investigation. 
Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` variables).  
In total `r length(id.all)` genes (or `r length(id.all)/dim(GeneExpression)[2]*100` %) had non-zero loadings. 
Figure \ref{slda-loadings} shows a plot of the loadings vs. gene index number. 
We could decide to only look at the genes with the largest (in absolute value) loadings and only consider those genes whose loadings are more than 2 standard deviations away from 0 (above the red line in figure \ref{slda-loadings}). 
These are the following `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r id.all[which(abs(loadings) > 2*sd(loadings))] %>% colnames(GeneExpression) %>% cat()`. 
Figure \ref{slda-scores} shows the discriminant scores from which we can see there is a substantial overlap between the 2 groups, suggesting that (sparse) LDA is not the best approach.

```{r slda-loadings}
plot(id.all, loadings)
abline(h=0)
abline(h=c(-2,2)*sd(loadings), col =2)
```

```{r slda-scores}
Z <- GeneExpression_S[,id.all] %*% loadings

plot(Z, col=RejectionStatus$Reject_Status+1, pch=16)
```

### LLE (locally linear embedding)

Locally linear embedding is a nonlinear dimension reduction method which finds a low-dimensional representation for each points' local neighbourhood by linearly approximating the data in each neighbourhood and returing these coordinates of lower dimension (Roweis and Saul, 2000).

```{r lle, eval=FALSE}
lle <- RDRToolbox::LLE(data=GeneExpression_C, dim=3, k=50)
labels = c("first component", "second component", 'third component')
plot(lle[,1],lle[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
```

From figures \ref{lle} can be seen that no distinction between the accepted and rejected kidneys can be made.

### ISOMAP

ISOMAP is a a nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000. It preserves rather the global properties of the data. It uses  multidimensional scaling but then with incorporating the geodesic distances imposed by a weighted graph of the k neighbours of each point.

```{r ISOMAP, eval=FALSE}
IM <- RDRToolbox::Isomap(data=GeneExpression_C, dims=3, k=30)
labelsIM <- c("first component", "second component", "third component")
plot(IM$dim3[,1],IM$dim3[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3d plot (remove in final version)
# RDRToolbox::plotDR(data=IM$dim3, labels=RejectionStatus[,2], axesLabels=labelsIM)

RDRToolbox::Isomap(data=GeneExpression_C, dims=1:10, k=30, plotResiduals=TRUE)
```

The parameter k is varied manually so that the maps are optimal. From figure \ref{ISOMAP} can be seen that with ISOMAP, it is also not possible to make a distinction between the group of accepted and rejected kidneys.


### Sammon mapping

Sammon mapping is also a nonlinear dimension reduction method. The cost function of the Sammon method is similar to that of MDS, except that it is adapted by dividing the squared error in the representation of each pairwise Euclidean distance by the original Euclidean distance in the high-dimensional space. In this way, local structure is better preserved than in MDS. The result is in figures \ref{sammon}: no distinction can be made between the two groups.

```{r sammon}
sammon <- MASS::sammon(dist(GeneExpression_C), k=3, niter=100)
plot(sammon$points[,1], sammon$points[,2], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
plot(sammon$points[,2], sammon$points[,3], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```

### Diffusion maps

Diffusion mapping is also a nonlinear dimension reduction method based on the eigenvectors and eigenvalues of the data. The global strucure of the data is mapped by integration of local similarities at different scales. It works with probablities of point x randomly walking to y, and is based on the heat diffusion equation.

```{r diffusion}
DiffusionMap <- diffusionMap::diffuse(dist(GeneExpression_C), maxdim=3)
plot(DiffusionMap$X, type='p', col=RejectionStatus$Reject_Status+1, pch=19)
```

From figure \ref{diffusion}, no distinction between the two groups can be made with diffusion maps.

### t-SNE

t-stochastic neighbor embedding is a dimension reduction technique for visualising high dimensional data with a focus on preserving the local structure. The resulting plot can be seen in figure \ref{tsne} and indicates again that a simple distinction between the two groups cannot be made. Yet, there seems to be roughly two groups that differ in heterogeneity: one largely heterogeneous group (upwards left in the plot) and one group that is less heterogeneous, though far from homogeneous (middle to downward right in the plot).

```{r tsne, fig.cap="Two dimensional representation of the data through application of t-SNE"}
gene_dist <- dist(GeneExpression_C)
tsne_Z <- tsne(gene_dist, k = 2, perplexity = 45)
tibble(as.data.frame(tsne_Z), reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", "Rejected"))) %>%
  ggplot(aes(V1, V2, color = reject)) +
  geom_point() +
  theme_classic() +
  labs(color = "Status",
       x="",
       y="")
```

## QQ-plots


```{r QQ plots for normality check}

Rejected <- matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus), 1)){
  if (RejectionStatus[[i,2]]==0){
    Rejected <- rbind(Rejected, GeneExpression_C[i,])
  }
}

Accepted <- matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==1){
    Accepted <- rbind(Accepted, GeneExpression_C[i,])
  }
}

Selection <- sample(seq(1, ncol(GeneExpression_C)), 3)

for (j in Selection) {
    qqPlot(Rejected[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in rejected kidneys'))
    qqPlot(Accepted[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in accepted kidneys'))
}
```

# References

Benjamini Y and Hochberg Y, 1995. Controlling The False Discovery Rate - A Practical And Powerful Approach To Multiple Testing. Journal of the Royal Statistical Society. Series B: Methodological 57, 289-300.

Roweis ST and Saul LK, 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290, 2323-2326.

Tenenbaum JB, De Silva V and Langford JC, 2000. A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319-2323.

