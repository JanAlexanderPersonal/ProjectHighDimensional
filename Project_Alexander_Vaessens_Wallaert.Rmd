---
title: | 
        | Project
        | Transplant kidney rejection
        | High Dimensional Data Analysis
author:
  - Jan Alexander^[jan.alexander@ugent.be]
  - Annabel Vaessens^[annabel.vaessens@vub.be]
  - Steven Wallaert^[steven.wallaert@ugent.be]
date: "8/4/2020"
output:
  pdf_document: 
    number_sections: yes
    toc: no
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{amsmath}
---

```{r setup, include=FALSE}

rm(list = ls())

library(ggplot2)
library(latex2exp)
library(tidyverse)
library(boot)
library(PMA)
library(pls)
library(MASS)
library(glmnet)
library(ROCR)
library(multtest)
library(RDRToolbox)
library(diffusionMap)
library(car)

knitr::opts_chunk$set(echo = TRUE, fig.width=6, fig.height=4)

set.seed(321)
```

```{r load data, eval=TRUE, include=TRUE}
load('RejectionStatus.rda')
load('X_GSE21374.rda')
dim(RejectionStatus)
dim(X_GSE21374)

GeneExpression <- t(X_GSE21374)
GeneExpression_C <- scale(t(X_GSE21374),scale = F)
GeneExpression_S <- scale(t(X_GSE21374),scale = T)
# vraagje: zie cursus p 74, men geeft aan voor PCA dat
# indien vars in zelfde eenheden zijn (in ons geval is dit zo), dan beter niet schalen.
# Zouden we dan mss beter niet schalen voor PCA?

GeneExpression <-
  GeneExpression[order(as.numeric(row.names(GeneExpression))), ]
RejectionStatus <-
  RejectionStatus[order(as.numeric(RejectionStatus$Patient_ID)), ]

all.equal(row.names(GeneExpression), as.character(RejectionStatus$Patient_ID))
```

# Introduction

The data is loaded as presented in the assignment. The data is loaded as the raw dataset, the centered dataset and the standardized dataset. 

Three research questions are defined:

* How do the 54675 genes vary in terms of their gene expression levels? Is the variability associated with kidney rejection? (only to be answered in a data explorative manner).
* Which genes are differentially expressed between the two kidney rejection groups? You must control the False Discovery Rate at 10%.
* Can the kidney rejection be predicted from the gene expressions? What genes are most important in predicting the kidney transplant rejection? How well does the prediction model perform in terms of predicting rejection status?

# Data exploration

In the complete dataset, `r round(mean(RejectionStatus$Reject_Status) * 100) ` % of the transplanted kidnesy were rejected.

## Simple plot of the raw data

Before plotting dimension reduced representations of the data, a simple plot of the raw data is made. (insert Stevens code)

```{r}


```

## Sparse principle components analysis

```{r sparse pca}
Gen_spc <- PMA::SPC(GeneExpression_C, K = 2, sumabsv = 2)
Uk <- Gen_spc$u ; Dk <- diag(Gen_spc$d)
Zk <- data.frame(X = Uk %*% Dk, Patient_ID = row.names(GeneExpression_C))
Zk <- merge(Zk, RejectionStatus, by = 'Patient_ID') %>% 
  mutate(Reject_Status = as.factor(Reject_Status))
ggplot(data = Zk, aes(x = X.1, y = X.2, col = Reject_Status)) + 
  geom_point()

rm(Zk, Dk, Uk, X_GSE21374, Gen_spc)
```

Unfortunately, naive sparce principle component analysis does not seem to work well.

## Partial least squares:

```{r pls}
GeneExpression_comb <-
  list(genes = as.matrix(GeneExpression_C), Rejection = as.matrix(RejectionStatus$Reject_Status))
pls_model <- pls::plsr(genes ~ Rejection, data = GeneExpression_comb, validation = "CV")
```

## Multi-dimensional scaling:

```{r mds}
GeneExpression_C.svd <- svd(GeneExpression_C)

k <- 2
Uk <- GeneExpression_C.svd$u[,1:k]; Dk <- diag(GeneExpression_C.svd$d[1:k]) 
Vk <-GeneExpression_C.svd$v[,1:k]
Xk <- Uk %*% Dk %*% t(Vk)
Zk <- Uk %*% Dk

rownames(Zk) <- RejectionStatus[[2]]
rownames(Vk) <- colnames(GeneExpression_C)
ColnamesNull <- colnames(GeneExpression_C)
ColnamesNull[]<- "" 

biplot(Uk, Vk, xlabs=RejectionStatus[[2]], var.axes=FALSE, ylabs=ColnamesNull, cex=0.75)

# eventueel
plot(Zk[,1], Zk[,2], col=RejectionStatus$Reject_Status+1,pch=19)
```

In the biplot of the two first dimensions of the svd, no distinction can be made between rejected and accepted kidneys.

```{r plots mds}
totvar <- sum(GeneExpression_C.svd$d^2)/(nrow(GeneExpression_C)-1)
barplot(cumsum(GeneExpression_C.svd$d^2/(nrow(GeneExpression_C)-1)/totvar), names.arg=1:nrow(GeneExpression_C), ylab='cumulative prop. of total variance')
```

In the scree plot above it can be seen that the two first dimensions account for only 26% of the total variance in the dataset.

## LDA

In an exploratory fashion, we look at the results of an LDA, performed with the first 300 gene expressions. We chose to do this because LDA performed using all gene expressions did not work.

```{r lda}
GeneExpression_C.lda <- lda(GeneExpression_C[,1:300], grouping = RejectionStatus$Reject_Status)

V <- GeneExpression_C.lda$scaling
Z <- GeneExpression_C[,1:300] %*% V

plot(Z, col =  RejectionStatus$Reject_Status+1, pch= 16, cex = 1.5)

```

We can see a clear distinction between the 2 rejection status groups.

```{r plot lda}
plot(V, cex = 0.8, cex.axis = 1.5, cex.lab = 1.5, xlab = "gene ID",
     ylab = "loading")

abline(h=0)
abline(h=c(2,-2)*sd(V), col = 2)

id <- which(abs(V) > 2*sd(V))
```

The most contributing gene expressions are:

```{r}
colnames(GeneExpression_C[,id])
```

We repeated this for 243 folds of 225 gene expressions because

```{r}
243 * 225 == dim(GeneExpression_C)[2]
```

```{r lda sequential on all}
id.all <- vector("numeric")

for (i in 1:(dim(GeneExpression_C)[2]/225)){

  start <- 1 + (i-1)*225
  stop <- start + 224
  gene.lda <- lda(GeneExpression_C[,start:stop], grouping = RejectionStatus$Reject_Status)

  V <- gene.lda$scaling
  Z <- GeneExpression_C[,start:stop] %*% V


  id.all <- append(id.all, which(abs(V) > 3*sd(V)) - 1 + start) # -1 to start at 0

}
gene.lda <- lda(GeneExpression_C[,id.all], grouping = RejectionStatus$Reject_Status)

V <- gene.lda$scaling
Z <- GeneExpression_C[,id.all] %*% V
plot(Z, col =  RejectionStatus$Reject_Status+1, pch= 16, cex = 1.5)

```

```{r lda on all other output}
plot(V, cex = 0.8, cex.axis = 1.5, cex.lab = 1.5, xlab = "gene ID",
     ylab = "loading")

abline(h=0)
abline(h=c(2,-2)*sd(V), col = 2)

id <- which(abs(V) > 2*sd(V))

colnames(GeneExpression_C[,id.all][,id])
```


Capping off at +- 3SD, this resulted in `r length(id.all)` gene expressions.

Using all these, and only these gene expressions, an LDA delivers `r dim(GeneExpression[,id.all][,id])[2]` genes.

Why at 3SD? PAS OP UITVOEREN DUURT LANG, in plaats daarvan, zie plot in de map

```{r lda probeersel, eval=FALSE}
id.all <- vector("numeric")
bss_wss <- vector("numeric")
p <- vector("numeric")

for(j in 2:8){
  id.all <- vector("numeric")
for (i in 1:(dim(gene)[2]/225)){

  start <- 1 + (i-1)*225
  stop <- start + 224
  gene.lda <- lda(gene[,start:stop], grouping = RejectionStatus$Reject_Status)

  V <- gene.lda$scaling
  Z <- gene[,start:stop] %*% V


  id.all <- append(id.all, which(abs(V) > j*sd(V)) - 1 + start) # -1 to start at 0

}

gene.lda <- lda(gene[,id.all], grouping = RejectionStatus$Reject_Status)

V <- gene.lda$scaling
Z <- gene[,id.all] %*% V

a <- sum((Z[RejectionStatus$Reject_Status==1] - mean(Z[RejectionStatus$Reject_Status==1]))**2)
b <- sum((Z[RejectionStatus$Reject_Status==0] - mean(Z[RejectionStatus$Reject_Status==0]))**2)

within <- a+b


a <- sum((mean(Z[RejectionStatus$Reject_Status==0])-mean(Z))**2)
b <- sum((mean(Z[RejectionStatus$Reject_Status==1])-mean(Z))**2)
between <- a*sum(RejectionStatus$Reject_Status==0)+b*sum(RejectionStatus$Reject_Status==1)

bss_wss[j] <- between/within
p[j] <- length(id.all)
}

plot(1:7, bss_wss, "b")
```


```{r plot sd}
knitr::include_graphics("hoeveel_sd.png")
```

## LLE (locally linear embedding)

Locally linear embedding (Roweis and Saul, 2000) is a nonlinear dimension reduction method which finds a low-dimensional representation for each points' local neighbourhood by linearly approximating the data in each neighbourhood and returing these coordinates of lower dimension.

(Value of k (number of neighbours) could be optimised)

```{r}
lle <- RDRToolbox::LLE(data=GeneExpression_C, dim=3, k=50)
labels = c("first component", "second component", 'third component')
plot(lle[,1],lle[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3D interactive plot: (remove in final version)
plot <-RDRToolbox::plotDR(data=lle, labels=RejectionStatus[,2], axesLabels=labels)
```
But also with LLE, we cannot differentiate between the accepted and rejected kidneys.
(more on LLE: https://www.stat.cmu.edu/~cshalizi/350/lectures/14/lecture-14.pdf)

## ISOMAP

ISOMAP is a a nonlinear dimension reduction technique presented by Tenenbaum, Silva and Langford in 2000. It preserves rather the global properties of the data. It uses  multidimensional scaling but then with incorporating the geodesic distances imposed by a weighted graph of the k neighbours of each point.

```{r}
IM <- RDRToolbox::Isomap(data=GeneExpression_C, dims=3, k=30)
labelsIM <- c("first component", "second component", "third component")
plot(IM$dim3[,1],IM$dim3[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3d plot (remove in final version)
RDRToolbox::plotDR(data=IM$dim3, labels=RejectionStatus[,2], axesLabels=labelsIM)

RDRToolbox::Isomap(data=GeneExpression_C, dims=1:10, k=30, plotResiduals=TRUE)
```
The parameter k is varied manually so that the maps are optimal. The value k could be optimised.
With ISOMAP, it is also not possible to make a distinction between the group of accepted and rejected kidneys.

## Sammon mapping

Sammon mapping is also a nonlinear dimension reduction method. The cost function of the Sammon method is similar to that of MDS, except that it is adapted by dividing the squared error in the representation of each pairwise Euclidean distance by the original Euclidean distance in the high-dimensional space. In this way, local structure is better preserved than in MDS.

```{r}
sammon <- MASS::sammon(dist(GeneExpression_C), k=3, niter=100)
plot(sammon$points[,1], sammon$points[,2], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
plot(sammon$points[,2], sammon$points[,3], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```

## Diffusion maps

Diffusion mapping is also a nonlinear dimension reduction method based on the eigenvectors and eigenvalues of the data. The global strucure of the data is mapped by integration of local similarities at different scales. It works with probablities of point x randomly walking to y, and is based on the heat diffusion equation.

```{r}
DiffusionMap <- diffusionMap::diffuse(dist(GeneExpression_C), maxdim=3)
plot(DiffusionMap$X, type='p', col=RejectionStatus$Reject_Status+1, pch=19)
```

# Differentiating genes between kidney acceptance and rejection

All hypothesis tests are done on the centered, not-standardized data. 

First, a check of the variables follow a normal distribution for both the accepted and rejected kidneys. This is done by checking several QQ plots. 

remark 1: noticed the method is not BH after all. Will fix this later an use Stevens suggestion
remark 2: also test whether the variances of the two groups is unequal? (there will be some variables for sure where variance is unequal, therefore safer to do the Welch t-test rather than the student t-test


```{r QQ plots for normality check, for several random genes}

Rejected=matrix(,ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==0){
    Rejected <- rbind(Rejected, GeneExpression_C[i,])
  }
}

Accepted=matrix(,ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==1){
    Accepted <- rbind(Accepted, GeneExpression_C[i,])
  }
}

Selection <- sample(seq(1,ncol(GeneExpression_C)), 15)

for (j in Selection) {
    qqPlot(Rejected[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression)[j] , 
                      'in rejected kidneys'))
    qqPlot(Accepted[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression)[j] , 
                      'in accepted kidneys'))
}

```

The QQ plots show that most genes follow the normal distribution, but there are also genes that do not.

remark: maybe replace by a large-scale hypothesis test for normality later on and then decide for the Welch t-test or Wilcoxon Mann-Whitney (now both are in the code)

A two-sided two-sample Welch t-test is done on the centered data (not standardized) to determine whether the two groups (rejected and accepted kidneys) can be differentiated based on the gene data, whether the mean  of the two groups is different.The Welch t-test is done because of unequal variances/difference in amount of samples between the two groups.

The test takes one hour to run. Therefore, the resulting object is loaded instead.

```{r two-sided two-sample Welch t-test}
# testMTP_Welch-t <- multtest::MTP(X=t(GeneExpression_C), Y = RejectionStatus[,2], 
#    na.rm = TRUE, test = "t.twosamp.unequalvar", robust = FALSE, 
#    standardize = TRUE , alternative = "two.sided", typeone='fdr', 
#     fdr.method =       "conservative", alpha = 0.10)

```

Interpretation: after exercise session 5

A Wilcoxon Mann-Whitney test is done on the centered data, to determine whether the two groups follow the same distribution.

```{r Wilcoxon Mann-Whitney test}
# testMTP_WMW <- multtest::MTP(X=t(GeneExpression_C), Y = RejectionStatus[,2], 
#    na.rm = TRUE, test = "t.twosamp.equalvar", robust = FALSE, 
#    standardize = TRUE , alternative = "two.sided", typeone='fdr', 
#    fdr.method = "conservative", 
#    alpha = 0.10)
```

Interpretation: after exercise session 5

# Prediction of kidney transplant rejection

```{r train test split}
ind_train <-
  sample(seq_len(nrow(RejectionStatus_C)), size = floor(nrow(RejectionStatus_C) * 0.80))

Y_train <- as.matrix(RejectionStatus_C[ind_train, 'Reject_Status'])
X_train <- as.matrix(GeneExpression[ind_train,])
Y_test <- as.matrix(RejectionStatus_C[-ind_train, 'Reject_Status'])
X_test <- as.matrix(GeneExpression[-ind_train,])
```

## Lasso regression

```{r lasso}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 1,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance lasso}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 1,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```

This model uses 
`r length(unique(summary(coef(m))[-1,1])) ` 
of the features. 
This is a considerable dimensional reduction.
This is illustrated below. This figure shows the loadings of the 
`r length(unique(summary(coef(m))[-1,1])) ` 
selected values.

```{r summary lasso, fig.width=4, fig.height=3.5, eval=TRUE, echo=FALSE}
plot(
  summary(coef(m))[-1, 1],
  summary(coef(m))[-1, 3],
  cex = 1,
  pch = 3,
  xlab = 'feature' ,
  ylab = TeX(" $ \\hat{\\beta} $ ")
)
```


## Ridge regression

```{r ridge}
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 0,
    family = 'binomial',
    type.measure = "auc"
  )
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r m.cv$lambda.min`  , the area under the curve ( _AUC_ ) is maximal for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance ridge}
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 0,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```


## Principal component regression

```{r PCR CV}
# cost function for CV
MISERR <- function(obs, pred, cutoff = 0.5){
  ypred <- as.numeric(pred > cutoff)
  tab <- table(obs, ypred)
  miserr <- 1 - sum(diag(tab))/sum(tab)
  return(miserr)
}

max.n.comps <- 50 #random nr

cv.glm.pcr <- rep(NA, max.n.comps)

X_train.svd <- svd(X_train)

U <- X_train.svd$u
D <- diag(X_train.svd$d)
Z_train <- U%*%D

for(i in 1:max.n.comps){
  fitdata <- data.frame(Y_train, Z_train[,1:i])
  
  mod <- glm(Y_train ~ ., data = fitdata, family = "binomial")
  
  cv.glm.pcr[i] <- cv.glm(fitdata, mod, cost = MISERR, K = 10)$delta[1]
}

npc.min <- which.min(cv.glm.pcr)

plot(1:max.n.comps, cv.glm.pcr, type = "l")
abline(v=npc.min, col =2)

```


```{r PCR model}
V <- X_train.svd$v
Z_test <- X_test %*% V

fitdata <- data.frame(Y_train, Z_train[,1:npc.min])

mod <- glm(Y_train ~ ., data = fitdata)

preddata <- data.frame(Z_test[,1:npc.min])

pred_mod <- prediction(predict(mod, newdata = preddata), Y_test)

perf_mod <- performance(pred_mod, "sens", "fpr")

plot(perf_mod)
```

AUC is `r performance(pred_mod, "auc")@y.values`.