---
title: | 
        | Project
        | Transplant kidney rejection
        | High Dimensional Data Analysis
author:
  - Jan Alexander^[jan.alexander@ugent.be]
  - Annabel Vaessens^[annabel.vaessens@vub.be]
  - Steven Wallaert^[steven.wallaert@ugent.be]
date: "`r format(Sys.Date(), '%d %m %Y')`"
output:
  bookdown::pdf_document2: 
    fig_caption: yes
    number_sections: yes
    toc: yes
  html_document:
    df_print: paged
    toc: yes
    toc_depth: '3'
  word_document:
    toc: yes
    toc_depth: '3'
header-includes: \usepackage{amsmath} \renewcommand{\abstractname}{Executive summary}
classoption: twocolumn
abstract: "This research examines whether some genes are responsible for a patient's likelihood of rejecting a kidney after transplantation, for the  Gene Expression Omninibus (GEO) dataset. This dataset consists of gene expression levels of 54675 genes from 282 patients. Data exploration methods show there is no clear way to map the gene expression levels onto the kidney rejection statuses. In other words, only probabilistic claims can be made. From the 54675 genes, 18081 genes are identified as having a differential expression between the group of rejected and the group of accepted kidneys. The list of these genes can be found in ...(insert appendix) Kidney rejection can be predicted sufficiently from the gene expressions with 17 genes. The most inmportant genes in predicting rejection are in... (insert appendix). The prediction model (insert which model is best) perfoms the best and (say something about the performance)"
---

```{r setup, include=FALSE}

rm(list = ls())

library(latex2exp)
library(tidyverse)
library(boot)
library(PMA)
library(pls)
library(MASS)
library(glmnet)
library(ROCR)
library(RDRToolbox)
library(diffusionMap)
library(car)
library(tsne)
library(locfdr)
library(xtable)

knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.width=6, fig.height=4, cache = TRUE)
options(scipen = 9)
options(xtable.comment = FALSE)
options(xtable.sanitize.text.function=identity)
set.seed(321)
```



```{r load data, include=FALSE}
load('RejectionStatus.rda')
load('X_GSE21374.rda')
dim(RejectionStatus)
dim(X_GSE21374)

GeneExpression <- t(X_GSE21374)
GeneExpression_C <- scale(t(X_GSE21374),scale = F) # centered
GeneExpression_S <- scale(t(X_GSE21374),scale = T) # scaled

GeneExpression <-
  GeneExpression[order(as.numeric(row.names(GeneExpression))), ]
RejectionStatus <-
  RejectionStatus[order(as.numeric(RejectionStatus$Patient_ID)), ]

all.equal(row.names(GeneExpression), as.character(RejectionStatus$Patient_ID))
```
# Abbreviations

```{r, results = "asis"}
abbreviation <- c('AUC','IQR', 'LDA', 'LASSO', 'LLE', 'MDS', 'PCA', 'ROC curve', 'sd', 't-SNE')
meaning <-
  c('Area under the curve',
    'Interquartile range',
    'Linear discriminant analysis',
    'Least absolute shrinkage and \\\\ & selection operator',
    'Locally linear embedding',
    'Multi dimensional scaling',
    'Principle component analysis',
    'Receiver operating characteristic \\\\ & curve',
    'Standard deviation',
    't-distributed stochastic neighbor \\\\ & embedding')

abb_table <- data.frame(Abbreviation = abbreviation, Meaning = meaning)

# knitr::kable(abb_table,
#              align = 'l',
#              col.names = c('Abbreviation', 'Meaning'),format="latex")
print(xtable(abb_table), include.rownames=FALSE)
```


# Exploratory Analysis

In this section general descriptive statistics are given and multiple methods for high dimensional data exploration are used.

## Basic descriptive summary

In this study `r dim(GeneExpression)[2]` gene expression levels of `r dim(GeneExpression)[1]` samples were analysed. In total, `r sum(RejectionStatus$Reject_Status)` or `r round(mean(RejectionStatus$Reject_Status) * 100) `% of the transplanted kidneys were rejected. 

Several descriptive statistics (mean, sd, median, iqr, min, and max) were calculated for every gene and kidney rejection status combination. This resulted in 2 (accepted vs. rejected) distributions of every statistic across genes. Note that these statistics were only calculated to perform a visual inspection.

The resulted plot is presented in figure \@ref(fig:descriptives). From this figure we can see there are, at least on this level, differences between the two groups. Most notable are the mean and median expression levels which tend to be closer to the overall mean (across groups) expression levels in the _accepted_ group and more varying in the _rejected_ group. There seems to be more variablity in the measures of dispersion in the _rejected_ group. Finally there are minimal differences between the min/max expression level distributions, perhaps suggesting that gene expression levels in the _rejected_ group are slightly less extreme than in the _accepted_ group.

```{r descriptives, echo=FALSE, fig.cap="Descriptive statistics across grenes and between groups."}
df <- tibble(patient = RejectionStatus$Patient_ID, 
             reject = ifelse(RejectionStatus$Reject_Status == 1, "Rejected", "Accepted"), 
             as.data.frame(GeneExpression_C)) %>%
  pivot_longer(cols = c(-reject,-patient) , names_to = "gene", values_to = "expression")

# calculate summaries
df %>%
  group_by(reject, gene) %>%
  summarise(mean = mean(expression),
            sd = sd(expression),
            min = min(expression),
            max = max(expression),
            median = median(expression),
            iqr = IQR(expression)) -> basics
# long format  
basics %>%
  pivot_longer(-c(reject, gene), 
               names_to = "statistic", 
               values_to = "value") -> basics_long
# fix order of statistics for plot facet
basics_long %>%
  mutate(statistic = factor(statistic, 
                            levels = c("mean", "sd", "min",
                                       "median", "iqr", "max"))) -> basics_long
# plot
ggplot(basics_long, aes(value, fill = reject)) +
  geom_density(alpha = 0.5) +
  theme_classic() +
  labs(fill = "Rejection status") + 
  facet_wrap("statistic", scales = "free" ) +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(x="centered gene expression level") +
  theme(legend.position = "bottom")
```

## Advanced exploratory analyses

Multiple methods for exploration and visualisation of high dimensional data were applied (sparse PCA, MDS, sparse LDA, LLE, ISOMAP, Sammon Mapping, Diffusion maps, and t-SNE), yet without clear results. Because the sparse LDA gave the best results we discuss the results here and refer to the appendices (\@ref(mds) to \@ref(tsneheader)) for the results of the other techniques.

```{r sparse lda}
id.all <- numeric()
loadings <- numeric()
variables_per_part <- dim(GeneExpression)[2]/3
for(i in 1:3){
  
  start <- 1 + (i-1)*variables_per_part
  stop <- start + variables_per_part-1
  
  gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status)
  
  V <- gene_lda$scaling
  
  Z <- GeneExpression_S[,start:stop] %*% V
  
  lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5)
  
  sparse_lda_loadings <- as.vector(coef(lda_loadings))
  
  id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225)
  loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0])
}
```

The sparse LDA was performed to find potential candidate genes for future investigation. Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` genes). We considered this approach to be valid since we only used it as an exploratory tool.
In total `r length(id.all)` genes (or `r round(length(id.all)/dim(GeneExpression)[2]*100,1)`%) had non-zero loadings. 

Because this is still a substantial amount, only the genes with loadings in absolute value larger than two standard deviations were further considered ($|v_i| > 2sd(v),$ with $i =\{1,...,116 \}$, where $v_i$ is the ith loading). This resulted in a list of `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r id.all[which(abs(loadings) > 2*sd(loadings))]`.

By using these gene's loadings we calculated the scores of the linear discriminant for every sample and used these to construct the following graph. 

```{r slda-density, fig.cap="Density plot of linear discriminant scores based on the selected subset of genes."}
best_all <- id.all[which(abs(loadings) > 2*sd(loadings))]
best_116 <- which(abs(loadings) > 2*sd(loadings))

Z <- GeneExpression_C[,best_all] %*% loadings[best_116]


tibble(scores = Z, reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", 
                                                                             "Rejected"))) %>%
  ggplot(aes(x = scores, fill = reject)) +
  geom_density(alpha = 0.4) +
  theme_classic() +
  scale_fill_manual(values = c("#4DBBD5", "#E64B35")) +
  labs(fill = "Rejection status") +
  theme(legend.position = "bottom") +
  geom_linerange(inherit.aes = F, aes(x=scores), ymin = 0, ymax=0.05 )

```

From this graph we can see that to a degree a distinction can be made, albeit with a substantial overlap.

## Conclusions Exploratory Analysis

Although differences between groups could be found within the data, no articulate distinction between the rejection status groups could be made with any of the used methods. This finding suggests there is relevant information at the genetic level w.r.t. transplant kidney rejection, but more factors need to be taken into account in order to arrive at a better understanding.

The main directions of variability in the gene expression dataset do not coincide with the separation between the rejection status groups. Nevertheless, certain genes were identified as potentially closely related to the differentiation between the two groups using sparse LDA.

# Differentially expressed genes between kidney rejection groups

In order to find out which genes are differentially expressed between rejection status groups null hypotheses were tested against alternative hypotheses as follows. 

$$\left.\begin{aligned} H_{0,i}: \mu_{rejected, i} = \mu_{accepted,i} \\ H_{a,i}: \mu_{rejected, i}\neq \mu_{accepted,i} \end{aligned} \right \} \quad with \space i = \{1,\dots,54675\} $$

In these hypotheses $\mu_{rejected,i}$ and $\mu_{accepted,i}$ are the population means of the gene expression level of the ith gene in the rejected and accepted kidney rejection group respectively. Before testing, a visual inspection of 30 QQ plots, from 15 randomly drawn variables, was done to assess whether the variables follow a normal distribution for both groups separately. These QQ plots showed that some genes were normally distributed, but also that many genes were not. 

Nevertheless, two-sided Welch t-tests were performed on the uncentered data to determine whether the two groups can be differentiated based on the gene expression level for every gene. The choice for the Welch t-test is motivated by the presence of unequal variances between the two groups, even though not all genes were normally distributed. We included a small random subset 6 QQ plots in the appendix so the reader, if she/he wishes, can have a rough idea of the divergence from normality.

To address the multiple testing problem at this large scale (54675 simultaneous tests), the FDR is controlled at 0.10 through application of the method of Benjamini and Hochberg (1995).


```{r welch t-test}
AllTestResults <- matrix(nrow=ncol(GeneExpression),  
                        ncol=3 ) 
rownames(AllTestResults) <- colnames(GeneExpression)
colnames(AllTestResults) <- c('p-value', 'test-statistic', 'degreeFreedom')

for (i in seq(1,54675)){
  test <- t.test(GeneExpression[,i]~RejectionStatus[,2])
  AllTestResults[i,'p-value'] <- test$p.value
  AllTestResults[i,'test-statistic'] <- test$statistic
  AllTestResults[i,'degreeFreedom'] <- test$parameter
}
```

To summarise the results we constructed a histogram of the adjusted p-values or q-values. 

```{r histogram p-values, fig.cap="Histograms of regular and adjusted p-values. The dashed line indicates the threshold."}
adjusted <- p.adjust(AllTestResults[,"p-value"], method = "BH")
rejections <- sum(adjusted < 0.1)

# tibble(gene_index = 1:length(AllTestResults[,1]), pval = AllTestResults[,1],
#        adjusted = adjusted) %>%
#   pivot_longer(-gene_index, names_to = "type", values_to = "p.value") %>%
#   mutate(type = factor(type, levels = c("pval", "adjusted"), 
#                        labels = c("p-value", "q-value"))) -> df.p

df.p <- tibble(adjusted)

ggplot(df.p, aes(adjusted)) +
  geom_histogram(color="grey70") +
#  facet_grid(.~type) +
  theme_classic() +
  geom_vline(xintercept = 0.1, color = "red", linetype = "dashed") +
  labs(x="q-value",
       y = "frequency") +
  scale_x_continuous(breaks = c(0, 0.1, 0.25, 0.5, 0.75, 1))

```

The histogram shows a non-uniform distribution. More importantly, it tells there are many small values, indicating that for many genes the null hypothesis was rejected. Based on the q-values, there were `r rejections` rejected null hypotheses. As such we conclude that the mean gene expression differs between the accepted and rejected kidney groups for those `r rejections` genes. As the FDR is controlled at 10%, it is expected to have around `r round(rejections*.1,0)` false discoveries.

Next, the normalised test statistics (z-scores) are plotted and compared to the local false discovery rate.

```{r lfdr}
z_scores <- qnorm(pt(AllTestResults[,'test-statistic'],df=AllTestResults[i,'degreeFreedom']))
fdr <- locfdr(z_scores,plot=0,nulltype=0)$fdr
z.locfdr <- locfdr(z_scores, nulltype = 0, plot = 1)
```

Fromthe graph in figure \@ref(fig:lfdr) can be concluded that a small lfdr can be obtained for small and large z-scores (a lfdr smaller than 0.2 for z-scores smaller than -2 and larger than 2). For these z-scores, it is more likely that when rejecting the null hypothesis, a true discovery is made.

# Prediction of kidney transplant rejection

The objective of this final part is to construct a classifier for kidney acceptance or rejection based on the measured gene expressions.
Three approaches are compared: lasso, ridge regression and principle component regression. 
These approaches will be compared based on the AUC.
The final modelling approach is chosen as the one that shows the largest cross-validated AUC.
The cutoff is chosen based on the F1-score.
The dataset is split into a training (70% of the data) and test dataset (30% of the data).

```{r train test split}
ind_train <-
  sample(seq_len(nrow(RejectionStatus)), size = floor(nrow(RejectionStatus) * 0.70))

Y_train <- as.matrix(RejectionStatus[ind_train, 'Reject_Status'])
X_train <- as.matrix(GeneExpression_C[ind_train,])
Y_test <- as.matrix(RejectionStatus[-ind_train, 'Reject_Status'])
X_test <- as.matrix(GeneExpression_C[-ind_train,])
```

## Lasso regression

```{r lasso}
set.seed(321)
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 1,
    family = 'binomial',
    type.measure = "auc"
  )
mod_lasso <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 1,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_mod_lasso <-
  prediction(predict(
    mod_lasso,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

In the figure above, one can see that for $\gamma$ equal to   `r round(m.cv$lambda.min, digits = 2)`  , the area under the curve ( _AUC_ ) is maximal (`r round(performance(pred_mod_lasso, "auc")@y.values[[1]], digits = 3)`) for the train dataset based on a 10-fold cross-validation over the train dataset. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance lasso}
lasso_selection <- unique(summary(coef(mod_lasso))[-1, 1])
perf <- performance(pred_mod_lasso, 'sens', 'fpr')
plot(perf)
```

This model only uses 
`r length(lasso_selection) ` 
of the genes: `r lasso_selection`.
This is a considerable dimensional reduction.
This is illustrated below. This figure shows the loadings of the 
`r length(unique(summary(coef(mod_lasso))[-1,1])) ` 
selected values.

```{r summary lasso, eval=TRUE, echo=FALSE}
plot(
  summary(coef(mod_lasso))[-1, 1],
  summary(coef(mod_lasso))[-1, 3],
  cex = 1,
  pch = 3,
  xlab = 'gene index' ,
  ylab = TeX(" $ \\hat{\\beta} $ ")
)
```



## Ridge regression

```{r ridge, eval=TRUE, echo=FALSE}
set.seed(321)
m.cv <-
  cv.glmnet(
    x = X_train,
    y = Y_train,
    alpha = 0,
    family = 'binomial',
    type.measure = "auc"
  )
m <- glmnet(
  x = X_train,
  y = Y_train,
  alpha = 0,
  family = 'binomial',
  lambda = m.cv$lambda.min
)
pred_m <-
  prediction(predict(
    m,
    newx = X_test,
    type = 'response'
  ),
  Y_test)
plot(m.cv, xlab = TeX(" $ log(\\gamma ) $ "))
```

Likewise as for the Lasso regression, for $\gamma$ equal to `r round(m.cv$lambda.min, digits = 2)`, the optimal _AUC_ (`r round(performance(pred_m, "auc")@y.values[[1]], digits = 3)`) is obtained. 

The ROC curve, estimated with the cross-validation dataset, is shown below:

```{r performance ridge}
perf <- performance(pred_m, 'sens', 'fpr')
plot(perf)
```

## Principal component regression

```{r PCR CV, warning=FALSE}
set.seed(321)
trapezoid_integration <- function(x, y) {
  elements <- (y[-1] + y[-length(y)]) * abs(diff(x))
  sum(elements/2)
}
# cost function for CV
AUC_est <- function(obs, pred){
  obs <- factor(obs, levels = c(0,1))
  # AUC is estimated by calculating the sensitivity and the FPR for different values of cutoff C
  intervals <- 500
  sensitivity <- rep(NA, intervals+1)
  specificity <- rep(NA, intervals+1)
  for(i in seq(0, intervals)){
    cutoff <- i/intervals
    ypred <- factor(as.numeric(pred > cutoff), levels = c(0, 1))
    tab <- table(obs, ypred)
    TN <- tab[1]
    FN <- tab[2]
    FP <- tab[3]
    TP <- tab[4]
    sensitivity[i+1] <- TP / (TP + FN)
    specificity[i+1] <- TN / (FP + TN)
  }
  #plot(1-specificity, sensitivity)
  AUC <- trapezoid_integration(1-specificity, sensitivity)
  #cat(paste0('AUC estimate : ', AUC, '\n'))
  return(AUC)
}

max.n.comps <- 100 #random nr

cv.glm.pcr <- rep(NA, max.n.comps)
X_train.svd <- svd(X_train)

U <- X_train.svd$u
D <- diag(X_train.svd$d)
Z_train <- U %*% D

for (i in 1:max.n.comps) {
  fitdata <- data.frame(Y_train, Z_train[, 1:i])
  
  mod <- glm(Y_train ~ ., data = fitdata, family = "binomial")
  
  cv.glm.pcr[i] <-
    cv.glm(fitdata, mod, cost = AUC_est, K = 10)$delta[1]
}

plot(1:max.n.comps, cv.glm.pcr, type = "l", main = 'Cross validated AUC for PCA regression')
npc.min <- which.max(cv.glm.pcr)
npc.val <- cv.glm.pcr[npc.min]
abline(v = npc.min, col = 2)
```


```{r PCR model, warning=FALSE}
V <- X_train.svd$v
Z_test <- X_test %*% V
fitdata <- data.frame(Y_train, Z_train[,1:npc.min])
mod <- glm(Y_train ~ ., data = fitdata)
preddata <- data.frame(Z_test[,1:npc.min])
pred_mod <- prediction(predict(mod, newdata = preddata), Y_test)
perf_mod <- performance(pred_mod, "sens", "fpr")
plot(perf_mod)
```

AUC is `r round(performance(pred_mod, "auc")@y.values[[1]], digits = 3)`.

## Final model evaluation

```{r}
perf_f1 <- performance(pred_mod_lasso, "f")
plot(perf_f1)
pred <- predict(mod_lasso,newx = X_test,type = 'response')
cutoff <- perf_f1@x.values[[1]][which.max(perf_f1@y.values[[1]])]
abline(v=cutoff, col =2)
```


The model performance in terms of AUC is similar for the 3 models.
Since LASSO regression is the simplest model, this is preferred.
By chosing cutoff c = `r round(cutoff, 3)`, we can achieve a F1-score of `r round(max(perf_f1@y.values[[1]], na.rm = T), 3)`. 
This is represented on the following graph. 
Below, the F1 graph, the confusion matrix for the Lasso model with cutoff c = `r round(cutoff, 3)` is shown.

The confusion matrix shows no false negatives at all, but the number of false positives is high.

```{r, results= "asis"}

ypred <- factor(as.numeric(pred > cutoff), levels = c(0, 1))
tab <- table(Y_test, ypred)
colnames(tab) <- c('accept pred', 'reject pred')
rownames(tab) <- c('accept obs', 'reject obs')
#knitr::kable(tab, digits = 0, align = 'l', format = "latex")
xtable(tab)
```
This confusionmatrix clearly shows a sensitivity of `r round(tab[4] / (tab[3] + tab[4]), digits = 2)`, and a specificity of `r round(tab[1] / (tab[1] + tab[2]), digits = 2)`. 
In short, this predictor seems to strike a balance between both, but the performance is not perfect.
If a person tests positive, there is still a considerable chance the kidney will not be rejected.


This behaviour could be _explained_ (or at least understood a little better) when looking back at the exploratory analysis. 
Here it was already clear that the gene expressions of the patients with rejected kidneys overlap with those of the patients with accepted kidneys. 
Both are not perfectly separable.
 

# Conclusions

From the 54675 genes in the dataset, `r rejections` genes are differentially expressed between the two kidney groups, based on multi-scale Welch t-test at an FDR of 10%.

# Appendices

## Exploration methods for high dimensional data

### Sparse principle components analysis

```{r}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.width=6, fig.height=4, cache = TRUE, eval=FALSE)

```


```{r sparse pca, eval=FALSE}
#SPC.cv(GeneExpression_C)
Gen_spc <- PMA::SPC(GeneExpression_C, K = 2, sumabsv = 5)
Uk <- Gen_spc$u ; Dk <- diag(Gen_spc$d)
Zk <- data.frame(X = Uk %*% Dk, Patient_ID = row.names(GeneExpression_C))
Zk <- merge(Zk, RejectionStatus, by = 'Patient_ID') %>% 
  mutate(Reject_Status = as.factor(Reject_Status))
ggplot(data = Zk, aes(x = X.1, y = X.2, col = Reject_Status)) + 
  geom_point()

rm(Zk, Dk, Uk, X_GSE21374, Gen_spc)
```

Unfortunately, naive sparce principle component analysis cannot be used to make a distinction between the accepted and rejected kidneys.

### Multi-dimensional scaling: {#mds}

```{r mds, eval=FALSE}
GeneExpression_C.svd <- svd(GeneExpression_C)

k <- 3
Uk <- GeneExpression_C.svd$u[,1:k]; Dk <- diag(GeneExpression_C.svd$d[1:k]) 
Vk <-GeneExpression_C.svd$v[,1:k]
Xk <- Uk %*% Dk %*% t(Vk)
Zk <- Uk %*% Dk

rownames(Zk) <- RejectionStatus[[2]]
rownames(Vk) <- colnames(GeneExpression_C)
ColnamesNull <- colnames(GeneExpression_C)
ColnamesNull[]<- "" 

plot(Zk[,1], Zk[,2], col=RejectionStatus$Reject_Status+1,pch=19)
plot(Zk[,2], Zk[,3], col=RejectionStatus$Reject_Status+1,pch=19)
```

In the biplots of the three first dimensions of the svd, no distinction can be made between rejected and accepted kidneys.

```{r plots-mds, fig.cap="Scree plot", eval=FALSE}
totvar <- sum(GeneExpression_C.svd$d^2)/(nrow(GeneExpression_C)-1)
barplot(cumsum(GeneExpression_C.svd$d^2/(nrow(GeneExpression_C)-1)/totvar), names.arg=1:nrow(GeneExpression_C), ylab='cumulative prop. of total variance')
```

In the scree plot \@ref(fig:plots-mds) it can be seen that the two first dimensions account for only 25% of the total variance in the dataset and the first three dimensions for 29%. To account for 80% of the total variance, 120 dimensions are needed.


<!-- ### Sparse LDA -->

<!-- ```{r sparse lda} -->
<!-- id.all <- numeric() -->
<!-- loadings <- numeric() -->
<!-- for(i in 1:3){ -->

<!--   start <- 1 + (i-1)*18225 -->
<!--   stop <- start + 18224 -->

<!--   gene_lda <- lda(GeneExpression_S[,start:stop], grouping = RejectionStatus$Reject_Status) -->

<!--   V <- gene_lda$scaling -->

<!--   Z <- GeneExpression_S[,start:stop] %*% V -->

<!--   #plot(Z,col = 1+RejectionStatus$Reject_Status, pch=16, cex=1) -->

<!--   #boxplot(Z~RejectionStatus$Reject_Status) -->

<!--   lda_loadings <- cv.glmnet(GeneExpression_S[,start:stop], Z, alpha = 0.5, nfolds = 5) -->

<!--   sparse_lda_loadings <- as.vector(coef(lda_loadings)) -->

<!--   #SLDA <- GeneExpression_S[,start:stop] %*% sparse_lda_loadings[-1] -->

<!--   #boxplot(SLDA ~ RejectionStatus$Reject_Status) -->

<!--   id.all <- append(id.all, which(sparse_lda_loadings[-1]!=0) + (i-1)*18225) -->
<!--   loadings <- append(loadings, sparse_lda_loadings[-1][sparse_lda_loadings[-1]!=0]) -->
<!-- } -->
<!-- ``` -->

<!-- A sparse LDA was performed to find out which genes contribute the most in discriminating between the two rejection status groups.  -->
<!-- These genes could then be possible targets for further investigation.  -->
<!-- Due to computational constraints (our system ran out of memory) we needed to split the data set in 3 parts (each part consisting of 282 observations on `r dim(GeneExpression)[2]/3` variables).   -->
<!-- In total `r length(id.all)` genes (or `r length(id.all)/dim(GeneExpression)[2]*100` %) had non-zero loadings.  -->
<!-- Figure \ref{slda-loadings} shows a plot of the loadings vs. gene index number.  -->
<!-- We could decide to only look at the genes with the largest (in absolute value) loadings and only consider those genes whose loadings are more than 2 standard deviations away from 0 (above the red line in figure \ref{slda-loadings}).  -->
<!-- These are the following `r length(id.all[which(abs(loadings) > 2*sd(loadings))])` genes: `r id.all[which(abs(loadings) > 2*sd(loadings))] %>% colnames(GeneExpression) %>% cat()`.  -->
<!-- Figure \ref{slda-scores} shows the discriminant scores from which we can see there is a substantial overlap between the 2 groups, suggesting that (sparse) LDA is not the best approach. -->

<!-- ```{r slda-loadings} -->
<!-- plot(id.all, loadings) -->
<!-- abline(h=0) -->
<!-- abline(h=c(-2,2)*sd(loadings), col =2) -->
<!-- ``` -->

<!-- ```{r slda-scores} -->
<!-- Z <- GeneExpression_S[,id.all] %*% loadings -->

<!-- plot(Z, col=RejectionStatus$Reject_Status+1, pch=16) -->
<!-- ``` -->

### LLE

Locally linear embedding described by Roweis and Saul (2000) was performed. From the next figures can be seen that no distinction between the accepted and rejected kidneys can be made.

```{r lle, fig.cap="LLE", fig.pos="hold", out.width="50%", eval=FALSE}
lle <- RDRToolbox::LLE(data=GeneExpression_C, dim=3, k=50)
labels = c("first component", "second component", 'third component')
plot(lle[,1],lle[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(lle[,2],lle[,3],col=RejectionStatus$Reject_Status+1,pch=19)
```

### ISOMAP

ISOMAP presented by Tenenbaum, Silva and Langford in 2000 is performed. The parameter k is varied manually so that the maps are optimal. From figure \@ref(fig:ISOMAP) can be seen that with ISOMAP, it is also not possible to make a distinction between the group of accepted and rejected kidneys.

```{r ISOMAP, fig.cap="ISOMAP", fig.pos="hold", out.width="50%", eval=FALSE}
IM <- RDRToolbox::Isomap(data=GeneExpression_C, dims=3, k=30)
labelsIM <- c("first component", "second component", "third component")
plot(IM$dim3[,1],IM$dim3[,2],col=RejectionStatus$Reject_Status+1,pch=19)
plot(IM$dim3[,2],IM$dim3[,3],col=RejectionStatus$Reject_Status+1,pch=19)
# 3d plot (remove in final version)
# RDRToolbox::plotDR(data=IM$dim3, labels=RejectionStatus[,2], axesLabels=labelsIM)

RDRToolbox::Isomap(data=GeneExpression_C, dims=1:10, k=30, plotResiduals=TRUE)
```

### Sammon mapping

Sammon mapping presented by Sammon (1969). The result is in figures \@ref(fig:sammon): no distinction can be made between the two groups.

```{r sammon, fig.cap="Sammon mapping", fig.pos="hold", out.width="50%", eval=FALSE}
sammon <- MASS::sammon(dist(GeneExpression_C), k=3, niter=100)
plot(sammon$points[,1], sammon$points[,2], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
plot(sammon$points[,2], sammon$points[,3], type = "p", col=RejectionStatus$Reject_Status+1, pch=19)
```

### Diffusion maps

Diffusion mapping was presented by Nadler et al. (2006) and LAfon and Lee (2006). From figure \@ref(fig:diffusion), no distinction between the two groups can be made with diffusion maps.

```{r diffusion, fig.cap="Diffusion map", eval=FALSE}
DiffusionMap <- diffusionMap::diffuse(dist(GeneExpression_C), maxdim=3)
plot(DiffusionMap$X, type='p', col=RejectionStatus$Reject_Status+1, pch=19)
```

### t-SNE {#tsneheader}

t-stochastic neighbor embedding is presented by Van Den Maaten and Hilton (2008). The resulting plot can be seen in figure \@ref(fig:tsne) and indicates again that a simple distinction between the two groups cannot be made. Yet, there seems to be roughly two groups that differ in heterogeneity: one largely heterogeneous group and one group that is less heterogeneous, though far from homogeneous.

```{r tsne, fig.cap="Two dimensional representation of the data through application of t-SNE", eval=FALSE}
gene_dist <- dist(GeneExpression_C)
tsne_Z <- tsne(gene_dist, k = 2, perplexity = 45)
tibble(as.data.frame(tsne_Z), reject = factor(RejectionStatus$Reject_Status, labels = c("Accepted", "Rejected"))) %>%
  ggplot(aes(V1, V2, color = reject)) +
  geom_point() +
  theme_classic() +
  labs(color = "Status",
       x="",
       y="")
```

## QQ-plots


```{r QQ plots for normality check, eval=FALSE}

Rejected <- matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus), 1)){
  if (RejectionStatus[[i,2]]==0){
    Rejected <- rbind(Rejected, GeneExpression_C[i,])
  }
}

Accepted <- matrix(ncol=ncol(GeneExpression_C))
for (i in seq(1,nrow(RejectionStatus),1)){
  if (RejectionStatus[[i,2]]==1){
    Accepted <- rbind(Accepted, GeneExpression_C[i,])
  }
}

Selection <- sample(seq(1, ncol(GeneExpression_C)), 3)

for (j in Selection) {
    qqPlot(Rejected[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in rejected kidneys'))
    qqPlot(Accepted[,j], pch = 16, 
           main=paste('QQ plot for expression of ', colnames(GeneExpression_C)[j] , 
                      'in accepted kidneys'))
}
```

# References

Benjamini Y and Hochberg Y, 1995. Controlling The False Discovery Rate - A Practical And Powerful Approach To Multiple Testing. Journal of the Royal Statistical Society. Series B: Methodological 57, 289-300.

Lafon S and Lee AB, 2006. Diffusion maps and coarse-graining: A unified framework for dimensionality
reduction, graph partitioning, and data set parameterization. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 28, 1393–1403.

Nadler B, Lafon S, Coifman RR, and Kevrekidis IG, 2006. Diffusion maps, spectral clustering and
the reaction coordinates of dynamical systems. Applied and Computational Harmonic Analysis:
Special Issue on Diffusion Maps and Wavelets, 21, 113–127.

Roweis ST and Saul LK, 2000. Nonlinear dimensionality reduction by locally linear embedding. Science, 290, 2323-2326.

Sammon JW, 1969. A nonlinear mapping for data structure analysis. IEEE Transactions on Computers,
18, 401–409.

Tenenbaum JB, De Silva V and Langford JC, 2000. A global geometric framework for nonlinear dimensionality reduction. Science, 290, 2319-2323.

Van Der Maaten L and Hilton G, 2008. Visualising data using t-SNE. Journal of Machine Learning Research, 9, 2579-2605.

